<!DOCTYPE html>
<html lang="en" dir="ltr" itemscope itemtype="http://schema.org/WebPage">

<head>
    <link rel="icon" href="favicon.ico" type="image/x-icon">
    <link rel="apple-touch-icon" sizes="180x180" href="./apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./favicon-16x16.png">
    <link rel="manifest" href="./site.webmanifest">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="Content-Type-Script" content="text/javascript">
    <meta name="Content-Type-Style" content="text/css">
    <meta name="author" content="Computer Vision And Image Understanding Lab">
    <meta name="description" content="Computer Vision And Image Understanding Lab">
    <link href="./assets/css/vendors/vendors.css" rel="stylesheet" type="text/css">
    <link href="./assets/css/vendors/vendors-overwrites.css" rel="stylesheet" type="text/css">
    <link href="./assets/css/styles.css" rel="stylesheet" type="text/css">
    <link href="./assets/css/demo1.css" rel="stylesheet" type="text/css">
    <script src="./assets/js/vendors/jquery.min.js"></script>
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700|Play:300,400|Source+Code+Pro:300,400"
        rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Sintony:400,700&amp;subset=latin,greek,cyrillic"
        rel="stylesheet" type="text/css">
    <title>Publication</title>
</head>

<body class="fullwidth sticky-header">
    <div id="wrapper" class="regular-layout">
        <header id="header" class="trans dark sticky-dark">
            <div class="head-main">
                <div class="container">
                    <div class="logo-wrapper"><a href="http://csce.uark.edu/"><img src="./assets/img/UA_Logo.png"
                                alt="UArk Logo" class="logo-light ol-retina"><img src="./assets/img/UA_Logo.png"
                                alt="UArk Logo" class="logo-dark ol-retina"></a></div>
                    <div class="text-logo" style="font-size: 20pt;"><b style="font-size: 25pt;">C</b>omputer <b
                            style="font-size: 25pt;">V</b>ision and <b style="font-size: 25pt;">I</b>mage <b
                            style="font-size: 25pt;">U</b>nderstanding Lab</div>
                    <ul id="primary-menu">
                        <li><a href="./index.html" title="Home"><span>Home</span></a></li>
                        <li class="current-menu-item"><a href="./publication.html"
                                title="Publication"><span>Publication</span></a></li>
                        <li><a href="./people.html" title="People"><span>People</span></a></li>
                        <li><a href="./contact.html" title="Contact"><span>Contact</span></a></li>
                    </ul>
                    <div class="header-icons">
                        <div class="ol-mobile-trigger hamburger hamburger--elastic">
                            <div class="hamburger-box">
                                <div class="hamburger-inner"></div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </header>
        <section id="contents">
            <div class="head-wrapper">
                <div data-parallax-mode="mode-title" class="page-head hvh-30 parallax-layer">
                    <section data-img-src="./assets/img/bg/oldmain.jpg" data-parallax-mode="mode-3"
                        class="section parallax-layer ov-dark-alpha-80 owl-videobg owl-video-wrapper"></section>
                </div>
            </div>
            <section class="page-contents">
                <section id="main-area">
                    <div class="shadow"></div>
                    <section class="section bg-white section-narrow-2">
                        <div class="container">

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/LIAAD.png" alt="LIAAD">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">LIAAD: Lightweight Attentive Angular Distillation for
                                        Large-scale Age-Invariant Face Recognition</h4>
                                    <div class="details">
                                        <span>Neurocomputing 2023</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://arxiv.org/abs/2004.05085"><i
                                                    class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p>
                                                                Disentangled representations have been commonly adopted
                                                                to Age-invariant Face Recognition (AiFR) tasks.
                                                                However, these methods have reached some limitations
                                                                with (1) the requirement of large-scale face recognition
                                                                (FR) training data with age labels, which is limited in
                                                                practice; (2) heavy deep network architectures for high
                                                                performance;
                                                                and (3) their evaluations are usually taken place on
                                                                age-related face databases while neglecting the standard
                                                                large-scale FR
                                                                databases to guarantee robustness. This work presents a
                                                                novel Lightweight Attentive Angular Distillation (LIAAD)
                                                                approach to
                                                                Large-scale Lightweight AiFR that overcomes these
                                                                limitations. Given two high-performance heavy networks
                                                                as teachers
                                                                with different specialized knowledge, LIAAD introduces a
                                                                learning paradigm to efficiently distill the
                                                                age-invariant
                                                                attentive and angular knowledge from those teachers to a
                                                                lightweight student network making it more powerful with
                                                                higher
                                                                FR accuracy and robust against age factor. Consequently,
                                                                LIAAD approach is able to take the advantages of both FR
                                                                datasets
                                                                with and without age labels to train an AiFR model. Far
                                                                apart from prior distillation methods mainly focusing on
                                                                accuracy
                                                                and compression ratios in closed-set problems, our LIAAD
                                                                aims to solve the open-set problem, i.e. large-scale
                                                                face recognition.
                                                                Evaluations on LFW, IJB-B and IJB-C Janus, AgeDB and
                                                                MegaFace-FGNet with one million distractors have
                                                                demonstrated
                                                                the efficiency of the proposed approach on light-weight
                                                                structure. This work also presents a new longitudinal
                                                                face aging (LogiFace) database
                                                                for further studies in age-related facial problems in
                                                                future.
                                                            </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>Thanh-Dat Truong, Chi Nhan Duong, Kha Gia Quach, Ngan Le,
                                                                Tien D Bui, and Khoa Luu (2023).
                                                                LIAAD: Lightweight Attentive Angular Distillation for
                                                                Large-scale Age-Invariant Face Recognition.
                                                                Neurocomputing, 2023.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>@article{truong2023liaad,
                                                                title={LIAAD: Lightweight Attentive Angular Distillation
                                                                for Large-scale Age-Invariant Face Recognition},
                                                                author={Thanh-Dat Truong, Chi Nhan Duong, Kha Gia Quach,
                                                                Ngan Le, Tien D Bui, and Khoa Luu},
                                                                journal={Neurocomputing},
                                                                year={2023}
                                                                }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/FREDOM_First_Fig.jpg" alt="FREDOM">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">FREDOM: Fairness Domain Adaptation Approach to Semantic
                                        Scene Understanding</h4>
                                    <div class="details">
                                        <span>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
                                            Recognition (CVPR) 2023</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href=""><i class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p>Although Domain Adaptation in Semantic Scene Segmentation
                                                                has shown impressive improvement in recent years,
                                                                the fairness concerns in the domain adaptation have yet
                                                                to be well defined and addressed. In addition,
                                                                fairness is one of the most critical aspects when
                                                                deploying the segmentation models into human-related
                                                                real-world applications, e.g., autonomous driving, as
                                                                any unfair predictions could influence human safety.
                                                                In this paper, we propose a novel Fairness Domain
                                                                Adaptation (FREDOM) approach to semantic scene
                                                                segmentation.
                                                                In particular, from the proposed formulated fairness
                                                                objective, a new adaptation framework will be introduced
                                                                based on the fair treatment of class distributions.
                                                                Moreover, to generally model the context of structural
                                                                dependency,
                                                                a new conditional structural constraint is introduced to
                                                                impose the consistency of predicted segmentation.
                                                                Thanks to the proposed Conditional Structure Network,
                                                                the self-attention mechanism has sufficiently modeled
                                                                the structural information of segmentation.
                                                                Through the ablation studies, the proposed method has
                                                                shown the performance improvement of the segmentation
                                                                models
                                                                and promoted fairness in the model predictions. The
                                                                experimental results on the two standard benchmarks,
                                                                i.e., SYNTHIA to Cityscapes and GTA5 to Cityscapes, have
                                                                shown that our method achieved State-of-the-Art (SOTA)
                                                                performance.
                                                            </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>Thanh-Dat Truong, Ngan Le, Bhiksha Raj, Jackson Cothren,
                                                                and Khoa Luu (2023).
                                                                FREDOM: Fairness Domain Adaptation Approach to Semantic
                                                                Scene Understanding.
                                                                In Proceedings of the IEEE/CVF Conference on Computer
                                                                Vision and Pattern Recognition (CVPR), 2023.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>@article{truong2023freedom,
                                                                title={FREDOM: Fairness Domain Adaptation Approach to
                                                                Semantic Scene Understanding},
                                                                author={Thanh-Dat Truong, Ngan Le, Bhiksha Raj, Jackson
                                                                Cothren, and Khoa Luu},
                                                                journal={IEEE/CVF Conference on Computer Vision and
                                                                Pattern Recognition (CVPR)},
                                                                year={2023}
                                                                }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/DirecFormer.png" alt="DirecFormer">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">DirecFormer: A Directed Attention in Transformer Approach to
                                        Robust Action Recognition</h4>
                                    <div class="details">
                                        <span>The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
                                            2022</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://arxiv.org/abs/2203.10233"><i
                                                    class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p>
                                                                Human action recognition has recently become one of the
                                                                popular research topics in the computer vision
                                                                community.
                                                                Various 3D-CNN based methods have been presented to
                                                                tackle both the spatial and temporal dimensions in the
                                                                task
                                                                of video action recognition with competitive results.
                                                                However, these methods have suffered some fundamental
                                                                limitations such as lack of robustness and
                                                                generalization, e.g., how does the temporal ordering of
                                                                video frames affect the recognition results?
                                                                This work presents a novel end-to-end Transformer-based
                                                                Directed Attention (DirecFormer) framework for robust
                                                                action recognition.
                                                                The method takes a simple but novel perspective of
                                                                Transformer-based approach to understand the right order
                                                                of sequence actions.
                                                                Therefore, the contributions of this work are
                                                                three-fold. Firstly, we introduce the problem of ordered
                                                                temporal learning issues to the action recognition
                                                                problem.
                                                                Secondly, a new Directed Attention mechanism is
                                                                introduced to understand and provide attentions to human
                                                                actions in the right order.
                                                                Thirdly, we introduce the conditional dependency in
                                                                action sequence modeling that includes orders and
                                                                classes.
                                                                The proposed approach consistently achieves the
                                                                state-of-the-art (SOTA) results compared with the recent
                                                                action recognition methods,
                                                                on three standard large-scale benchmarks, i.e. Jester,
                                                                Kinetics-400 and Something-Something-V2.
                                                            </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>Thanh-Dat Truong, Quoc-Huy Bui, Chi Nhan Duong, Han-Seok
                                                                Seo, Son Lam Phung, Xin Li, and Khoa Luu (2022).
                                                                DirecFormer: A Directed Attention in Transformer
                                                                Approach to Robust Action Recognition.
                                                                In Proceedings of the IEEE/CVF Conference on Computer
                                                                Vision and Pattern Recognition (CVPR), 2022.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>@article{truong2022direcformer,
                                                                title={DirecFormer: A Directed Attention in Transformer
                                                                Approach to Robust Action Recognition},
                                                                author={Thanh-Dat Truong, Quoc-Huy Bui, Chi Nhan Duong,
                                                                Han-Seok Seo, Son Lam Phung, Xin Li, and Khoa Luu},
                                                                journal={IEEE/CVF Conference on Computer Vision and
                                                                Pattern Recognition (CVPR)},
                                                                year={2022}
                                                                }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/otadapt.png" alt="OTAdapt">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30"> OTAdapt: Optimal Transport-based Approach For Unsupervised
                                        Domain Adaptation </h4>
                                    <div class="details">
                                        <span>International Conference on Pattern Recognition (ICPR) 2022</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon"
                                                href="https://ieeexplore.ieee.org/iel7/9956007/9955631/09956335.pdf"><i
                                                    class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p> </p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/right2talk.png" alt="Right2Talk">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30"> The Right to Talk: An audio-visual Transformer Approach
                                    </h4>
                                    <div class="details">
                                        <span>Proceedings of the IEEE/CVF International Conference on Computer Vision
                                            (ICCV) 2021</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon"
                                                href="https://openaccess.thecvf.com/content/ICCV2021/papers/Truong_The_Right_To_Talk_An_Audio-Visual_Transformer_Approach_ICCV_2021_paper.pdf"><i
                                                    class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p> </p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/bimal.png" alt="BiMaL">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30"> BiMal: Bijective Maximum Likelihood Approach to Domain
                                        Adaptation in Semantic Scene Segmentation </h4>
                                    <div class="details">
                                        <span>Proceedings of the IEEE/CVF International Conference on Computer Vision
                                            (ICCV) 2021</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon"
                                                href="https://openaccess.thecvf.com/content/ICCV2021/papers/Truong_BiMaL_Bijective_Maximum_Likelihood_Approach_to_Domain_Adaptation_in_Semantic_ICCV_2021_paper.pdf"><i
                                                    class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p> </p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/invnxn.png" alt="InvNxN">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30"> Fast Flow Reconstruction via Robust Invertible NxN
                                        Convolution </h4>
                                    <div class="details">
                                        <span>Future Internet 2021</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon"
                                                href="https://www.mdpi.com/1999-5903/13/7/179/pdf"><i
                                                    class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p> </p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/graphcnn_analysis.jpeg" alt="GraphCNN">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30"> Movement Analysis for Neurological and Musculoskeletal
                                        Disorders Using Graph Convolutional Neural Network </h4>
                                    <div class="details">
                                        <span>Future Internet 2021</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon"
                                                href="https://www.mdpi.com/1999-5903/13/8/194/pdf"><i
                                                    class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p> </p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/vec2face.png" alt="Vec2Face">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30"> Vec2Face: Unveil Human Faces from their Blackbox Features
                                        in Face Recognition </h4>
                                    <div class="details">
                                        <span>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
                                            Recognition (CVPR) 2020</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon"
                                                href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Duong_Vec2Face_Unveil_Human_Faces_From_Their_Blackbox_Features_in_Face_CVPR_2020_paper.pdf"><i
                                                    class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p> </p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/unvp.png" alt="UNVP">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">Domain Generalization via Universal Non-volume Preserving
                                        Approach</h4>
                                    <div class="details">
                                        <span>Conference on Computer and Robot Vision (CRV) 2020</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon"
                                                href="https://ieeexplore.ieee.org/abstract/document/9108692"><i
                                                    class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p> </p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/micron-bert-abstract-figure.png"
                                        alt="Micron-BERT">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">Micron-BERT: BERT-based Facial Micro-Expression Recognition
                                    </h4>
                                    <div class="details">
                                        <span>The IEEE/CVF Conference on Computer Vision and Pattern Recognition
                                            2023</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="#"><i class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p>Micro-expression recognition is one of the most
                                                                challenging topics in affective computing. It aims to
                                                                recognize tiny facial movements difficult for humans to
                                                                perceive in a brief period, i.e., 0.25 to 0.5 seconds.
                                                                Recent advances in pre-training deep Bidirectional
                                                                Transformers (BERT) have significantly improved
                                                                self-supervised learning tasks in computer vision.
                                                                However, the standard BERT in vision problems is
                                                                designed to learn only from full images or videos, and
                                                                the architecture cannot accurately detect details of
                                                                facial micro-expressions. This paper presents
                                                                Micron-BERT, a novel approach to facial micro-expression
                                                                recognition. The proposed method can automatically
                                                                capture these movements in an unsupervised manner based
                                                                on two key ideas. First, we employ Diagonal
                                                                Micro-Attention (DMA) to detect tiny differences between
                                                                two frames. Second, we introduce a new Patch of Interest
                                                                (PoI) module to localize and highlight micro-expression
                                                                interest regions and simultaneously reduce noisy
                                                                backgrounds and distractions. By incorporating these
                                                                components into an end-to-end deep network, the proposed
                                                                -BERT significantly outperforms all previous work in
                                                                various micro-expression tasks.
                                                                -BERT can be trained on a large-scale unlabeled dataset,
                                                                i.e., up to 8 million images, and achieves high accuracy
                                                                on new unseen facial micro-expression datasets.
                                                                Empirical experiments show
                                                                -BERT consistently outperforms state-of-the-art
                                                                performance on four micro-expression benchmarks,
                                                                including SAMM, CASME II, SMIC, and CASME3, by
                                                                significant margins. Code will be available at
                                                                https://github.com/uark-cviu/Micron-BERT.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>Nguyen, Xuan-Bac and Duong, Chi Nhan and Xin, Li
                                                                and Susan, Gauch and Han-Seok, Seo and Luu, Khoa (2023).
                                                                Micron-BERT: BERT-based Facial Micro-Expression
                                                                Recognition.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>@INPROCEEDINGS{nguyen2023micronbert,
                                                                author={Nguyen, Xuan-Bac and Duong, Chi Nhan and Xin, Li
                                                                and Susan, Gauch and Han-Seok, Seo and Luu, Khoa},
                                                                booktitle={2023 IEEE/CVF Conference on Computer Vision
                                                                and Pattern Recognition (CVPR)},
                                                                title={Micron-BERT: BERT-based Facial Micro-Expression
                                                                Recognition},
                                                                year={2023}
                                                                }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>



                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/clusformer-abstract-figure.png"
                                        alt="Clusformer">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">Clusformer: A transformer based clustering approach to
                                        unsupervised large-scale face and visual landmark recognition
                                    </h4>
                                    <div class="details">
                                        <span>The IEEE/CVF Conference on Computer Vision and Pattern Recognition
                                            2021</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon"
                                                href="https://openaccess.thecvf.com/content/CVPR2021/papers/Nguyen_Clusformer_A_Transformer_Based_Clustering_Approach_to_Unsupervised_Large-Scale_Face_CVPR_2021_paper.pdf?ref=https://githubhelp.com"><i
                                                    class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p>The research in automatic unsupervised visual clustering
                                                                has received considerable attention over the last couple
                                                                years. It aims at explaining distributions of unlabeled
                                                                visual images by clustering them via a parameterized
                                                                model of appearance. Graph Convolutional Neural Networks
                                                                (GCN) have recently been one of the most popular
                                                                clustering methods. However, it has reached some
                                                                limitations. Firstly, it is quite sensitive to hard or
                                                                noisy samples. Secondly, it is hard to investigate with
                                                                various deep network models due to its computational
                                                                training time. Finally, it is hard to design an
                                                                end-to-end training model between the deep feature
                                                                extraction and GCN clustering modeling. This work
                                                                therefore presents the Clusformer, a simple but new
                                                                perspective of Transformer based approach, to automatic
                                                                visual clustering via its unsupervised attention
                                                                mechanism. The proposed method is able to robustly deal
                                                                with noisy or hard samples. It is also flexible and
                                                                effective to collaborate with different deep network
                                                                models with various model sizes in an end-to-end
                                                                framework. The proposed method is evaluated on two
                                                                popular large-scale visual databases, ie Google Landmark
                                                                and MS-Celeb-1M face database, and outperforms prior
                                                                unsupervised clustering methods. Code will be available
                                                                at https://github. com/VinAIResearch/Clusformer</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>Nguyen XB, Bui DT, Duong CN, Bui TD, Luu K. Clusformer: A
                                                                Transformer based Clustering Approach to Unsupervised
                                                                Large-scale Face and Visual Landmark Recognition. In2021
                                                                IEEE/CVF Conference on Computer Vision and Pattern
                                                                Recognition (CVPR) 2021 Jun 1 (pp. 10842-10851). IEEE
                                                                Computer Society.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>@InProceedings{Nguyen_2021_CVPR,
                                                                author = {Nguyen, Xuan-Bac and Bui, Duc Toan and Duong,
                                                                Chi Nhan and Bui, Tien D. and Luu, Khoa},
                                                                title = {Clusformer: A Transformer Based Clustering
                                                                Approach to Unsupervised Large-Scale Face and Visual
                                                                Landmark Recognition},
                                                                booktitle = {Proceedings of the IEEE/CVF Conference on
                                                                Computer Vision and Pattern Recognition (CVPR)},
                                                                month = {June},
                                                                year = {2021},
                                                                pages = {10847-10856}
                                                                }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>


                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/quantum-2d.png" alt="Clusformer">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">Two-dimensional quantum material identification via
                                        self-attention and soft-labeling in deep learning
                                    </h4>
                                    <div class="details">
                                        <span>arXiv preprint arXiv:2205.15948</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://arxiv.org/pdf/2205.15948"><i
                                                    class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p>In quantum machine field, detecting two-dimensional (2D)
                                                                materials in Silicon chips is one of the most critical
                                                                problems. Instance segmentation can be considered as a
                                                                potential approach to solve this problem. However,
                                                                similar to other deep learning methods, the instance
                                                                segmentation requires a large scale training dataset and
                                                                high quality annotation in order to achieve a
                                                                considerable performance. In practice, preparing the
                                                                training dataset is a challenge since annotators have to
                                                                deal with a large image, e.g 2K resolution, and
                                                                extremely dense objects in this problem. In this work,
                                                                we present a novel method to tackle the problem of
                                                                missing annotation in instance segmentation in 2D
                                                                quantum material identification. We propose a new
                                                                mechanism for automatically detecting false negative
                                                                objects and an attention based loss strategy to reduce
                                                                the negative impact of these objects contributing to the
                                                                overall loss function. We experiment on the 2D material
                                                                detection datasets, and the experiments show our method
                                                                outperforms previous works.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>Nguyen XB, Bisht A, Churchill H, Luu K. Two-dimensional
                                                                quantum material identification via self-attention and
                                                                soft-labeling in deep learning. arXiv preprint
                                                                arXiv:2205.15948. 2022 May 31.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>@article{nguyen2022two,
                                                                title={Two-dimensional quantum material identification
                                                                via self-attention and soft-labeling in deep learning},
                                                                author={Nguyen, Xuan Bac and Bisht, Apoorva and
                                                                Churchill, Hugh and Luu, Khoa},
                                                                journal={arXiv preprint arXiv:2205.15948},
                                                                year={2022}
                                                                }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/saga-tracknet.jpg" alt="SAGA-TrackNet">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">Multi-Camera Multi-Object Tracking on the Move via
                                        Single-Stage Global Association Approach</h4>
                                    <div class="details">
                                        <span>arXiv preprint arXiv:2211.09663</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://arxiv.org/abs/2211.09663"><i
                                                    class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p>The development of autonomous vehicles generates a
                                                                tremendous demand for a low-cost solution with a
                                                                complete set of camera
                                                                sensors capturing the environment around the car. It is
                                                                essential for object
                                                                detection and tracking to address these new challenges
                                                                in multi-camera
                                                                settings. In order to address these challenges, this
                                                                work introduces novel
                                                                Single-Stage Global Association Tracking approaches to
                                                                associate one or more
                                                                detection from multi-cameras with tracked objects. These
                                                                approaches aim to
                                                                solve fragment-tracking issues caused by inconsistent 3D
                                                                object detection.
                                                                Moreover, our models also improve the detection accuracy
                                                                of the standard
                                                                vision-based 3D object detectors in the nuScenes
                                                                detection challenge. The
                                                                experimental results on the nuScenes dataset demonstrate
                                                                the benefits of the
                                                                proposed method by outperforming prior vision-based
                                                                tracking methods in
                                                                multi-camera settings.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>Nguyen, P., Quach, K. G., Duong, C. N., Phung, S. L., Le,
                                                                N., & Luu, K. (2022). Multi-Camera Multi-Object Tracking
                                                                on
                                                                the Move via Single-Stage Global Association Approach.
                                                                arXiv preprint arXiv:2211.09663.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>@article{nguyen2022multi,
                                                                title={Multi-Camera Multi-Object Tracking on the Move
                                                                via Single-Stage Global Association Approach},
                                                                author={Nguyen, Pha and Quach, Kha Gia and Duong, Chi
                                                                Nhan and Phung, Son Lam and Le, Ngan and Luu, Khoa},
                                                                journal={arXiv preprint arXiv:2211.09663},
                                                                year={2022}
                                                                }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/SODE.png" alt="MC-MOT"></div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">Depth Perspective-aware Multiple Object Tracking</h4>
                                    <div class="details">
                                        <span>arXiv preprint arXiv:2207.04551</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://arxiv.org/abs/2207.04551"><i
                                                    class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p>This paper aims to tackle Multiple Object Tracking (MOT),
                                                                an important problem in computer vision but remains
                                                                challenging due to many practical issues, especially
                                                                occlusions. Indeed, we propose a new real-time Depth
                                                                Perspective-aware Multiple Object Tracking (DP-MOT)
                                                                approach to tackle the occlusion problem in MOT. A
                                                                simple yet
                                                                efficient Subject-Ordered Depth Estimation (SODE) is
                                                                first proposed to automatically order the depth
                                                                positions of
                                                                detected subjects in a 2D scene in an unsupervised
                                                                manner. Using the output from SODE, a new Active
                                                                pseudo-3D Kalman
                                                                filter, a simple but effective extension of Kalman
                                                                filter with dynamic control variables, is then proposed
                                                                to
                                                                dynamically update the movement of objects. In addition,
                                                                a new high-order association approach is presented in
                                                                the
                                                                data association step to incorporate first-order and
                                                                second-order relationships between the detected objects.
                                                                The
                                                                proposed approach consistently achieves state-of-the-art
                                                                performance compared to recent MOT methods on standard
                                                                MOT
                                                                benchmarks.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>Nguyen, P., Quach, K. G., Duong, C. N., Phung, S. L., Le,
                                                                N., & Luu, K. (2022). Multi-Camera Multi-Object Tracking
                                                                on
                                                                the Move via Single-Stage Global Association Approach.
                                                                arXiv preprint arXiv:2211.09663.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>@article{nguyen2022multi,
                                                                title={Multi-Camera Multi-Object Tracking on the Move
                                                                via Single-Stage Global Association Approach},
                                                                author={Nguyen, Pha and Quach, Kha Gia and Duong, Chi
                                                                Nhan and Phung, Son Lam and Le, Ngan and Luu, Khoa},
                                                                journal={arXiv preprint arXiv:2211.09663},
                                                                year={2022}
                                                                }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/graph_mcmot.png" alt="MC-MOT"></div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">Multi-Camera Multiple 3D Object Tracking on the Move for
                                        Autonomous Vehicles</h4>
                                    <div class="details">
                                        <span>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
                                            Recognition (CVPR) Workshops</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon"
                                                href="https://openaccess.thecvf.com/content/CVPR2022W/Precognition/html/Nguyen_Multi-Camera_Multiple_3D_Object_Tracking_on_the_Move_for_Autonomous_CVPRW_2022_paper.html"><i
                                                    class="fa fa-file"></i>Paper</a> |
                                            <a class="link-with-icon" href="./docs/cvprw22_poster.pdf"><i
                                                    class="fa fa-image"></i>Poster</a> |
                                            <a class="link-with-icon"
                                                href="https://drive.google.com/file/d/1SUw-_JkwMqRHgU6apNMHWEnf6FynBe4r/view?usp=sharing"><i
                                                    class="fa fa-video-camera"></i>Video</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p>The development of autonomous vehicles provides an
                                                                opportunity to have a complete set of camera sensors
                                                                capturing the
                                                                environment around the car. Thus, it is important for
                                                                object detection and tracking to address new challenges,
                                                                such
                                                                as achieving consistent results across views of cameras.
                                                                To address these challenges, this work presents a new
                                                                Global Association Graph Model with Link Prediction
                                                                approach to predict existing tracklets location and link
                                                                detections with tracklets via cross-attention motion
                                                                modeling and appearance re-identification. This approach
                                                                aims
                                                                at solving issues caused by inconsistent 3D object
                                                                detection. Moreover, our model exploits to improve the
                                                                detection
                                                                accuracy of a standard 3D object detector in the
                                                                nuScenes detection challenge. The experimental results
                                                                on the
                                                                nuScenes dataset demonstrate the benefits of the
                                                                proposed method to produce SOTA performance on the
                                                                existing
                                                                vision-based tracking dataset.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>Nguyen, P., Quach, K. G., Duong, C. N., Phung, S. L., Le,
                                                                N., & Luu, K. (2022). Multi-Camera Multi-Object Tracking
                                                                on
                                                                the Move via Single-Stage Global Association Approach.
                                                                arXiv preprint arXiv:2211.09663.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>@article{nguyen2022multi,
                                                                title={Multi-Camera Multi-Object Tracking on the Move
                                                                via Single-Stage Global Association Approach},
                                                                author={Nguyen, Pha and Quach, Kha Gia and Duong, Chi
                                                                Nhan and Phung, Son Lam and Le, Ngan and Luu, Khoa},
                                                                journal={arXiv preprint arXiv:2211.09663},
                                                                year={2022}
                                                                }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/selfsup_counting.png"
                                        alt="Crowd Counting"></div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">Self-supervised Domain Adaptation in Crowd Counting</h4>
                                    <div class="details">
                                        <span>2022 IEEE International Conference on Image Processing (ICIP)</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="./projects/crowd-counting.html"><i
                                                    class="fa fa-link"></i>Project Page</a> |
                                            <a class="link-with-icon" href="https://arxiv.org/abs/2206.03431"><i
                                                    class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                            <p>Self-training crowd counting has not been attentively
                                                                explored though it is one of the important challenges in
                                                                computer vision. In practice, the fully supervised
                                                                methods usually require an intensive resource of manual
                                                                annotation. In order to address this challenge, this
                                                                work introduces a new approach to utilize existing
                                                                datasets
                                                                with ground truth to produce more robust predictions on
                                                                unlabeled datasets, named domain adaptation, in crowd
                                                                counting. While the network is trained with labeled
                                                                data, samples without labels from the target domain are
                                                                also
                                                                added to the training process. In this process, the
                                                                entropy map is computed and minimized in addition to the
                                                                adversarial training process designed in
                                                                parallel.<br><br>Experiments on Shanghaitech, UCF_CC_50,
                                                                and UCF-QNRF
                                                                datasets prove a more generalized improvement of our
                                                                method over the other state-of-the-arts in the
                                                                cross-domain
                                                                setting.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>Nguyen, P., Quach, K. G., Duong, C. N., Phung, S. L., Le,
                                                                N., & Luu, K. (2022). Multi-Camera Multi-Object Tracking
                                                                on
                                                                the Move via Single-Stage Global Association Approach.
                                                                arXiv preprint arXiv:2211.09663.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                            <p>@article{nguyen2022multi,
                                                                title={Multi-Camera Multi-Object Tracking on the Move
                                                                via Single-Stage Global Association Approach},
                                                                author={Nguyen, Pha and Quach, Kha Gia and Duong, Chi
                                                                Nhan and Phung, Son Lam and Le, Ngan and Luu, Khoa},
                                                                journal={arXiv preprint arXiv:2211.09663},
                                                                year={2022}
                                                                }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </section>
                </section>
                <div class="clearfix"></div>
            </section>
        </section>
        <footer id="footer" class="dark-wrapper">
            <div id="footer-bar">
                <div class="container">
                    <div class="row tb-vcenter-wrapper bottom-bar">
                        <div class="col-sm-8 vcenter">
                            <ul class="footer-menu">
                                <li><a href="./index.html">Home</a></li>
                                <li><a href="./publication.html"><b>Publication</b></a></li>
                                <li><a href="./people.html">People</a></li>
                                <li><a href="./contact.html">Contact</a></li>
                            </ul>
                            <div class="sp-blank-10"></div>
                            <div class="inline-wrapper">
                                <div class="copyright"><i class="fa fa-copyright"></i> CVIU Lab
                                    <script>document.write(new Date().getFullYear())</script>.
                                </div>
                            </div>
                        </div>
                        <div class="col-sm-4 vcenter footer-socials">
                            <ul class="social-icons border-circle hover-tb-theme text-right-sm">
                                <li><a href="mailto:khoaluu@uark.edu"><i class="fa fa-envelope"></i></a></li>
                                <li><a href="tel:+14126085708"><i class="fa fa-phone"></i></a></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </footer>
    </div>
    <script src="./assets/js/vendors/vendors.js"></script>
    <script src="./assets/js/custom.js"></script>
</body>

</html>