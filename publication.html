<!DOCTYPE html>
<html lang="en" dir="ltr" itemscope itemtype="http://schema.org/WebPage">

<head>
    <link rel="icon" href="favicon.ico" type="image/x-icon">
    <link rel="apple-touch-icon" sizes="180x180" href="./apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./favicon-16x16.png">
    <link rel="manifest" href="./site.webmanifest">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="Content-Type-Script" content="text/javascript">
    <meta name="Content-Type-Style" content="text/css">
    <meta name="author" content="Computer Vision And Image Understanding Lab">
    <meta name="description" content="Computer Vision And Image Understanding Lab">
    <link href="./assets/css/vendors/vendors.css" rel="stylesheet" type="text/css">
    <link href="./assets/css/vendors/vendors-overwrites.css" rel="stylesheet" type="text/css">
    <link href="./assets/css/styles.css" rel="stylesheet" type="text/css">
    <link href="./assets/css/demo1.css" rel="stylesheet" type="text/css">
    <script src="./assets/js/vendors/jquery.min.js"></script>
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700|Play:300,400|Source+Code+Pro:300,400"
        rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Sintony:400,700&amp;subset=latin,greek,cyrillic"
        rel="stylesheet" type="text/css">
    <title>Publication</title>
</head>

<body class="fullwidth sticky-header">
    <div id="wrapper" class="regular-layout">
        <header id="header" class="trans dark sticky-dark">
            <div class="head-main">
                <div class="container">
                    <div class="logo-wrapper"><a href="http://csce.uark.edu/"><img src="./assets/img/UA_Logo.png"
                                alt="UArk Logo" class="logo-light ol-retina"><img src="./assets/img/UA_Logo.png"
                                alt="UArk Logo" class="logo-dark ol-retina"></a></div>
                    <div class="text-logo" style="font-size: 20pt;"><b style="font-size: 25pt;">C</b>omputer <b
                            style="font-size: 25pt;">V</b>ision and <b style="font-size: 25pt;">I</b>mage <b
                            style="font-size: 25pt;">U</b>nderstanding Lab</div>
                    <ul id="primary-menu">
                        <li><a href="./index.html" title="Home"><span>Home</span></a></li>
                        <li class="current-menu-item"><a href="./publication.html"
                                title="Publication"><span>Publication</span></a></li>
                        <li><a href="./people.html" title="People"><span>People</span></a></li>
                        <li><a href="./contact.html" title="Contact"><span>Contact</span></a></li>
                    </ul>
                    <div class="header-icons">
                        <div class="ol-mobile-trigger hamburger hamburger--elastic">
                            <div class="hamburger-box">
                                <div class="hamburger-inner"></div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </header>
        <section id="contents">
            <div class="head-wrapper">
                <div data-parallax-mode="mode-title" class="page-head hvh-30 parallax-layer">
                    <section data-img-src="./assets/img/bg/oldmain.jpg" data-parallax-mode="mode-3"
                        class="section parallax-layer ov-dark-alpha-80 owl-videobg owl-video-wrapper"></section>
                </div>
            </div>
            <section class="page-contents">
                <section id="main-area">
                    <div class="shadow"></div>
                    <section class="section bg-white section-narrow-2">
                        <div class="container">

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/LIAAD.png" alt="LIAAD">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">LIAAD: Lightweight Attentive Angular Distillation for Large-scale Age-Invariant Face Recognition</h4>
                                    <div class="details">
                                        <span>Neurocomputing 2023</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://arxiv.org/abs/2004.05085"><i class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                        <p>
                                                        Disentangled representations have been commonly adopted to Age-invariant Face Recognition (AiFR) tasks.
                                                         However, these methods have reached some limitations with (1) the requirement of large-scale face recognition 
                                                         (FR) training data with age labels, which is limited in practice; (2) heavy deep network architectures for high performance;
                                                          and (3) their evaluations are usually taken place on age-related face databases while neglecting the standard large-scale FR 
                                                          databases to guarantee robustness. This work presents a novel Lightweight Attentive Angular Distillation (LIAAD) approach to 
                                                          Large-scale Lightweight AiFR that overcomes these limitations. Given two high-performance heavy networks as teachers 
                                                          with different specialized knowledge, LIAAD introduces a learning paradigm to efficiently distill the age-invariant
                                                           attentive and angular knowledge from those teachers to a lightweight student network making it more powerful with higher 
                                                           FR accuracy and robust against age factor. Consequently, LIAAD approach is able to take the advantages of both FR datasets 
                                                           with and without age labels to train an AiFR model. Far apart from prior distillation methods mainly focusing on accuracy 
                                                           and compression ratios in closed-set problems, our LIAAD aims to solve the open-set problem, i.e. large-scale face recognition. 
                                                           Evaluations on LFW, IJB-B and IJB-C Janus, AgeDB and MegaFace-FGNet with one million distractors have demonstrated 
                                                           the efficiency of the proposed approach on light-weight structure. This work also presents a new longitudinal face aging (LogiFace) database 
                                                           for further studies in age-related facial problems in future.
                                                        </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p>Thanh-Dat Truong, Chi Nhan Duong, Kha Gia Quach, Ngan Le, Tien D Bui, and Khoa Luu (2023).
                                                            LIAAD: Lightweight Attentive Angular Distillation for Large-scale Age-Invariant Face Recognition.
                                                            Neurocomputing, 2023.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p>@article{truong2023liaad,
                                                            title={LIAAD: Lightweight Attentive Angular Distillation for Large-scale Age-Invariant Face Recognition},
                                                            author={Thanh-Dat Truong, Chi Nhan Duong, Kha Gia Quach, Ngan Le, Tien D Bui, and Khoa Luu},
                                                            journal={Neurocomputing},
                                                            year={2023}
                                                        }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/FREDOM_First_Fig.jpg" alt="FREDOM">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding</h4>
                                    <div class="details">
                                        <span>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href=""><i class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                        <p>Although Domain Adaptation in Semantic Scene Segmentation has shown impressive improvement in recent years, 
                                                            the fairness concerns in the domain adaptation have yet to be well defined and addressed. In addition, 
                                                            fairness is one of the most critical aspects when deploying the segmentation models into human-related 
                                                            real-world applications, e.g., autonomous driving, as any unfair predictions could influence human safety. 
                                                            In this paper, we propose a novel Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation. 
                                                            In particular, from the proposed formulated fairness objective, a new adaptation framework will be introduced 
                                                            based on the fair treatment of class distributions. Moreover, to generally model the context of structural dependency, 
                                                            a new conditional structural constraint is introduced to impose the consistency of predicted segmentation. 
                                                            Thanks to the proposed Conditional Structure Network, the self-attention mechanism has sufficiently modeled the structural information of segmentation. 
                                                            Through the ablation studies, the proposed method has shown the performance improvement of the segmentation models 
                                                            and promoted fairness in the model predictions. The experimental results on the two standard benchmarks, 
                                                            i.e., SYNTHIA to Cityscapes and GTA5 to Cityscapes, have shown that our method achieved State-of-the-Art (SOTA) performance.
                                                        </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p>Thanh-Dat Truong, Ngan Le, Bhiksha Raj, Jackson Cothren, and Khoa Luu (2023).
                                                            FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding.
                                                            In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p>@article{truong2023freedom,
                                                            title={FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding},
                                                            author={Thanh-Dat Truong, Ngan Le, Bhiksha Raj, Jackson Cothren, and Khoa Luu},
                                                            journal={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
                                                            year={2023}
                                                        }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/DirecFormer.png" alt="DirecFormer">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition</h4>
                                    <div class="details">
                                        <span>The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2022</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://arxiv.org/abs/2203.10233"><i class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                        <p>
                                                            Human action recognition has recently become one of the popular research topics in the computer vision community. 
                                                            Various 3D-CNN based methods have been presented to tackle both the spatial and temporal dimensions in the task 
                                                            of video action recognition with competitive results. However, these methods have suffered some fundamental 
                                                            limitations such as lack of robustness and generalization, e.g., how does the temporal ordering of video frames affect the recognition results?
                                                            This work presents a novel end-to-end Transformer-based Directed Attention (DirecFormer) framework for robust action recognition. 
                                                            The method takes a simple but novel perspective of Transformer-based approach to understand the right order of sequence actions.  
                                                            Therefore, the contributions of this work are three-fold. Firstly, we introduce the problem of ordered temporal learning issues to the action recognition problem. 
                                                            Secondly, a new Directed Attention mechanism is introduced to understand and provide attentions to human actions in the right order. 
                                                            Thirdly, we introduce the conditional dependency in action sequence modeling that includes orders and classes. 
                                                            The proposed approach consistently achieves the state-of-the-art (SOTA) results compared with the recent action recognition methods, 
                                                            on three standard large-scale benchmarks, i.e. Jester, Kinetics-400 and Something-Something-V2.
                                                        </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p>Thanh-Dat Truong, Quoc-Huy Bui, Chi Nhan Duong, Han-Seok Seo, Son Lam Phung, Xin Li, and Khoa Luu (2022).
                                                            DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition.
                                                            In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p>@article{truong2022direcformer,
                                                            title={DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition},
                                                            author={Thanh-Dat Truong, Quoc-Huy Bui, Chi Nhan Duong, Han-Seok Seo, Son Lam Phung, Xin Li, and Khoa Luu},
                                                            journal={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
                                                            year={2022}
                                                        }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/otadapt.png" alt="OTAdapt">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30"> OTAdapt: Optimal Transport-based Approach For Unsupervised Domain Adaptation </h4>
                                    <div class="details">
                                        <span>International Conference on Pattern Recognition (ICPR) 2022</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://ieeexplore.ieee.org/iel7/9956007/9955631/09956335.pdf"><i class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                        <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p> </p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/right2talk.png" alt="Right2Talk">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30"> The Right to Talk: An audio-visual Transformer Approach </h4>
                                    <div class="details">
                                        <span>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) 2021</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Truong_The_Right_To_Talk_An_Audio-Visual_Transformer_Approach_ICCV_2021_paper.pdf"><i class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                        <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p> </p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/bimal.png" alt="BiMaL">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30"> BiMal: Bijective Maximum Likelihood Approach to Domain Adaptation in Semantic Scene Segmentation </h4>
                                    <div class="details">
                                        <span>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) 2021</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Truong_BiMaL_Bijective_Maximum_Likelihood_Approach_to_Domain_Adaptation_in_Semantic_ICCV_2021_paper.pdf"><i class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                        <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p> </p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/invnxn.png" alt="InvNxN">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30"> Fast Flow Reconstruction via Robust Invertible NxN Convolution </h4>
                                    <div class="details">
                                        <span>Future Internet 2021</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://www.mdpi.com/1999-5903/13/7/179/pdf"><i class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                        <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p> </p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/graphcnn_analysis.jpeg" alt="GraphCNN">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30"> Movement Analysis for Neurological and Musculoskeletal Disorders Using Graph Convolutional Neural Network </h4>
                                    <div class="details">
                                        <span>Future Internet 2021</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://www.mdpi.com/1999-5903/13/8/194/pdf"><i class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                        <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p> </p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/vec2face.png" alt="Vec2Face">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30"> Vec2Face: Unveil Human Faces from their Blackbox Features in Face Recognition </h4>
                                    <div class="details">
                                        <span>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2020</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Duong_Vec2Face_Unveil_Human_Faces_From_Their_Blackbox_Features_in_Face_CVPR_2020_paper.pdf"><i class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                        <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p> </p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/unvp.png" alt="UNVP">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">Domain Generalization via Universal Non-volume Preserving Approach</h4>
                                    <div class="details">
                                        <span>Conference on Computer and Robot Vision (CRV) 2020</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://ieeexplore.ieee.org/abstract/document/9108692"><i class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                        <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p> </p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p> </p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/saga-tracknet.jpg" alt="SAGA-TrackNet">
                                </div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">Multi-Camera Multi-Object Tracking on the Move via
                                        Single-Stage Global Association Approach</h4>
                                    <div class="details">
                                        <span>arXiv preprint arXiv:2211.09663</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://arxiv.org/abs/2211.09663"><i class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                        <p>The development of autonomous vehicles generates a
                                                            tremendous demand for a low-cost solution with a complete set of camera
                                                            sensors capturing the environment around the car. It is essential for object
                                                            detection and tracking to address these new challenges in multi-camera
                                                            settings. In order to address these challenges, this work introduces novel
                                                            Single-Stage Global Association Tracking approaches to associate one or more
                                                            detection from multi-cameras with tracked objects. These approaches aim to
                                                            solve fragment-tracking issues caused by inconsistent 3D object detection.
                                                            Moreover, our models also improve the detection accuracy of the standard
                                                            vision-based 3D object detectors in the nuScenes detection challenge. The
                                                            experimental results on the nuScenes dataset demonstrate the benefits of the
                                                            proposed method by outperforming prior vision-based tracking methods in
                                                            multi-camera settings.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p>Nguyen, P., Quach, K. G., Duong, C. N., Phung, S. L., Le, N., & Luu, K. (2022). Multi-Camera Multi-Object Tracking on
                                                            the Move via Single-Stage Global Association Approach. arXiv preprint arXiv:2211.09663.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p>@article{nguyen2022multi,
                                                            title={Multi-Camera Multi-Object Tracking on the Move via Single-Stage Global Association Approach},
                                                            author={Nguyen, Pha and Quach, Kha Gia and Duong, Chi Nhan and Phung, Son Lam and Le, Ngan and Luu, Khoa},
                                                            journal={arXiv preprint arXiv:2211.09663},
                                                            year={2022}
                                                        }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/SODE.png" alt="MC-MOT"></div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">Depth Perspective-aware Multiple Object Tracking</h4>
                                    <div class="details">
                                        <span>arXiv preprint arXiv:2207.04551</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://arxiv.org/abs/2207.04551"><i class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                        <p>This paper aims to tackle Multiple Object Tracking (MOT), an important problem in computer vision but remains
                                                            challenging due to many practical issues, especially occlusions. Indeed, we propose a new real-time Depth
                                                            Perspective-aware Multiple Object Tracking (DP-MOT) approach to tackle the occlusion problem in MOT. A simple yet
                                                            efficient Subject-Ordered Depth Estimation (SODE) is first proposed to automatically order the depth positions of
                                                            detected subjects in a 2D scene in an unsupervised manner. Using the output from SODE, a new Active pseudo-3D Kalman
                                                            filter, a simple but effective extension of Kalman filter with dynamic control variables, is then proposed to
                                                            dynamically update the movement of objects. In addition, a new high-order association approach is presented in the
                                                            data association step to incorporate first-order and second-order relationships between the detected objects. The
                                                            proposed approach consistently achieves state-of-the-art performance compared to recent MOT methods on standard MOT
                                                            benchmarks.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p>Nguyen, P., Quach, K. G., Duong, C. N., Phung, S. L., Le, N., & Luu, K. (2022). Multi-Camera Multi-Object Tracking on
                                                            the Move via Single-Stage Global Association Approach. arXiv preprint arXiv:2211.09663.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p>@article{nguyen2022multi,
                                                            title={Multi-Camera Multi-Object Tracking on the Move via Single-Stage Global Association Approach},
                                                            author={Nguyen, Pha and Quach, Kha Gia and Duong, Chi Nhan and Phung, Son Lam and Le, Ngan and Luu, Khoa},
                                                            journal={arXiv preprint arXiv:2211.09663},
                                                            year={2022}
                                                        }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/graph_mcmot.png" alt="MC-MOT"></div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">Multi-Camera Multiple 3D Object Tracking on the Move for
                                        Autonomous Vehicles</h4>
                                    <div class="details">
                                        <span>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="https://openaccess.thecvf.com/content/CVPR2022W/Precognition/html/Nguyen_Multi-Camera_Multiple_3D_Object_Tracking_on_the_Move_for_Autonomous_CVPRW_2022_paper.html"><i class="fa fa-file"></i>Paper</a> |
                                            <a class="link-with-icon" href="./docs/cvprw22_poster.pdf"><i class="fa fa-image"></i>Poster</a> |
                                            <a class="link-with-icon" href="https://drive.google.com/file/d/1SUw-_JkwMqRHgU6apNMHWEnf6FynBe4r/view?usp=sharing"><i class="fa fa-video-camera"></i>Video</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                        <p>The development of autonomous vehicles provides an opportunity to have a complete set of camera sensors capturing the
                                                            environment around the car. Thus, it is important for object detection and tracking to address new challenges, such
                                                            as achieving consistent results across views of cameras. To address these challenges, this work presents a new
                                                            Global Association Graph Model with Link Prediction approach to predict existing tracklets location and link
                                                            detections with tracklets via cross-attention motion modeling and appearance re-identification. This approach aims
                                                            at solving issues caused by inconsistent 3D object detection. Moreover, our model exploits to improve the detection
                                                            accuracy of a standard 3D object detector in the nuScenes detection challenge. The experimental results on the
                                                            nuScenes dataset demonstrate the benefits of the proposed method to produce SOTA performance on the existing
                                                            vision-based tracking dataset.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p>Nguyen, P., Quach, K. G., Duong, C. N., Phung, S. L., Le, N., & Luu, K. (2022). Multi-Camera Multi-Object Tracking on
                                                            the Move via Single-Stage Global Association Approach. arXiv preprint arXiv:2211.09663.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p>@article{nguyen2022multi,
                                                            title={Multi-Camera Multi-Object Tracking on the Move via Single-Stage Global Association Approach},
                                                            author={Nguyen, Pha and Quach, Kha Gia and Duong, Chi Nhan and Phung, Son Lam and Le, Ngan and Luu, Khoa},
                                                            journal={arXiv preprint arXiv:2211.09663},
                                                            year={2022}
                                                        }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-2 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/selfsup_counting.png"
                                        alt="Crowd Counting"></div>
                                <div class="col-md-10">
                                    <h4 class="m-bottom-30">Self-supervised Domain Adaptation in Crowd Counting</h4>
                                    <div class="details">
                                        <span>2022 IEEE International Conference on Image Processing (ICIP)</span>
                                        <span class="block-elem" style="font-family:Source Code Pro"><br>
                                            <a class="link-with-icon" href="./projects/crowd-counting.html"><i class="fa fa-link"></i>Project Page</a> |
                                            <a class="link-with-icon" href="https://arxiv.org/abs/2206.03431"><i class="fa fa-file"></i>Paper</a>
                                        </span>
                                    </div>
                                    <div class="ol-accordion toggle-free">
                                        <div class="ac-item">
                                            <div class="item-head"><b>More details</b></div>
                                            <div class="item-body">
                                                <div class="ol-tab">
                                                    <ul class="tab-navigation">
                                                        <li class="active"><a href="#">abstract</a></li>
                                                        <li><a href="#">cite</a></li>
                                                        <li><a href="#">bibtex</a></li>
                                                    </ul>
                                                    <div class="tab-content details">
                                                        <div class="tab-pane active text-justify">
                                                        <p>Self-training crowd counting has not been attentively explored though it is one of the important challenges in
                                                            computer vision. In practice, the fully supervised methods usually require an intensive resource of manual
                                                            annotation. In order to address this challenge, this work introduces a new approach to utilize existing datasets
                                                            with ground truth to produce more robust predictions on unlabeled datasets, named domain adaptation, in crowd
                                                            counting. While the network is trained with labeled data, samples without labels from the target domain are also
                                                            added to the training process. In this process, the entropy map is computed and minimized in addition to the
                                                            adversarial training process designed in parallel.<br><br>Experiments on Shanghaitech, UCF_CC_50, and UCF-QNRF
                                                            datasets prove a more generalized improvement of our method over the other state-of-the-arts in the cross-domain
                                                            setting.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p>Nguyen, P., Quach, K. G., Duong, C. N., Phung, S. L., Le, N., & Luu, K. (2022). Multi-Camera Multi-Object Tracking on
                                                            the Move via Single-Stage Global Association Approach. arXiv preprint arXiv:2211.09663.</p>
                                                        </div>
                                                        <div class="tab-pane text-justify">
                                                        <p>@article{nguyen2022multi,
                                                            title={Multi-Camera Multi-Object Tracking on the Move via Single-Stage Global Association Approach},
                                                            author={Nguyen, Pha and Quach, Kha Gia and Duong, Chi Nhan and Phung, Son Lam and Le, Ngan and Luu, Khoa},
                                                            journal={arXiv preprint arXiv:2211.09663},
                                                            year={2022}
                                                        }</p>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </section>
                </section>
                <div class="clearfix"></div>
            </section>
        </section>
        <footer id="footer" class="dark-wrapper">
            <div id="footer-bar">
                <div class="container">
                    <div class="row tb-vcenter-wrapper bottom-bar">
                        <div class="col-sm-8 vcenter">
                            <ul class="footer-menu">
                                <li><a href="./index.html">Home</a></li>
                                <li><a href="./publication.html"><b>Publication</b></a></li>
                                <li><a href="./people.html">People</a></li>
                                <li><a href="./contact.html">Contact</a></li>
                            </ul>
                            <div class="sp-blank-10"></div>
                            <div class="inline-wrapper">
                                <div class="copyright"><i class="fa fa-copyright"></i> CVIU Lab
                                    <script>document.write(new Date().getFullYear())</script>.
                                </div>
                            </div>
                        </div>
                        <div class="col-sm-4 vcenter footer-socials">
                            <ul class="social-icons border-circle hover-tb-theme text-right-sm">
                                <li><a href="mailto:khoaluu@uark.edu"><i class="fa fa-envelope"></i></a></li>
                                <li><a href="tel:+14126085708"><i class="fa fa-phone"></i></a></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </footer>
    </div>
    <script src="./assets/js/vendors/vendors.js"></script>
    <script src="./assets/js/custom.js"></script>
</body>

</html>