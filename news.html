<!DOCTYPE html>
<html lang="en" dir="ltr" itemscope itemtype="http://schema.org/WebPage">

<head>
    <link rel="icon" href="favicon.ico" type="image/x-icon">
    <link rel="apple-touch-icon" sizes="180x180" href="./apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./favicon-16x16.png">
    <link rel="manifest" href="./site.webmanifest">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="Content-Type-Script" content="text/javascript">
    <meta name="Content-Type-Style" content="text/css">
    <meta name="author" content="Computer Vision And Image Understanding Lab">
    <meta name="description" content="Computer Vision And Image Understanding Lab">
    <link href="./assets/css/vendors/vendors.css" rel="stylesheet" type="text/css">
    <link href="./assets/css/vendors/vendors-overwrites.css" rel="stylesheet" type="text/css">
    <link href="./assets/css/styles.css" rel="stylesheet" type="text/css">
    <link href="./assets/css/demo1.css" rel="stylesheet" type="text/css">
    <script src="./assets/js/vendors/jquery.min.js"></script>
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700|Play:300,400|Source+Code+Pro:300,400"
        rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Sintony:400,700&amp;subset=latin,greek,cyrillic"
        rel="stylesheet" type="text/css">
    <title>Contact</title>
</head>

<body class="fullwidth sticky-header">
    <div id="wrapper" class="regular-layout">
        <header id="header" class="trans dark sticky-dark">
            <div class="head-main">
                <div class="container">
                    <div class="logo-wrapper"><a href="http://csce.uark.edu/"><img src="./assets/img/UA_Logo.png"
                                alt="UArk Logo" class="logo-light ol-retina"><img src="./assets/img/UA_Logo.png"
                                alt="UArk Logo" class="logo-dark ol-retina"></a></div>
                    <div class="text-logo" style="font-size: 20pt;"><b style="font-size: 25pt;">C</b>omputer <b
                            style="font-size: 25pt;">V</b>ision and <b style="font-size: 25pt;">I</b>mage <b
                            style="font-size: 25pt;">U</b>nderstanding Lab</div>
                    <ul id="primary-menu">
                        <li><a href="./index.html" title="Home"><span>Home</span></a></li>
                        <li><a href="./publication.html" title="Publication"><span>Publication</span></a></li>
                        <li><a href="https://github.com/uark-cviu" title="Data & Code"><span>Data & Code</span></a></li>
                        <li class="current-menu-item"><a href="./news.html"
                            title="News"><span>Projects & News</span></a></li>
                        <li><a href="./people.html" title="People"><span>People</span></a></li>
                        <li><a href="./contact.html" title="Contact"><span>Contact</span></a></li>
                    </ul>
                    <div class="header-icons">
                        <div class="ol-mobile-trigger hamburger hamburger--elastic">
                            <div class="hamburger-box">
                                <div class="hamburger-inner"></div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </header>
        <section id="contents">
            <div class="head-wrapper">
                <div data-parallax-mode="mode-title" class="page-head hvh-30 parallax-layer">
                    <section data-img-src="./assets/img/bg/oldmain.jpg" data-parallax-mode="mode-3"
                        class="section parallax-layer ov-dark-alpha-80 owl-videobg owl-video-wrapper"></section>
                </div>
            </div>
            <section class="page-contents">
                <section id="main-area">
                    <div class="shadow"></div>
                    <section class="section bg-white section-narrow-2">
                        <div class="container">

                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1">
                                    <img src="./projects/arabidopsis-thaliana/static/images/hyperedge-figure-5-caption.png" alt="">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Model Plant Arabidopsis thaliana</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">The genome of an organism offers a wealth of information vital to understanding how it reacts to outside stimuli. One such model plant, called Arabidopsis thaliana, has been critical to developing biology researchers' understanding of other plants, including crop plants. Meanwhile, in recent years, hypergraphs and deep learning have seen substantial progress toward understanding datasets with higher-order relationships. Gene data and relationships within the genome fit well into the hypergraph structure, where its hyperedges naturally represent biological functions genes are known to contribute to. This is more intuitive than pair-wise edges in simple graphs, where connecting genes is not as straightforward. To the best of our knowledge, there is no work that connects hypergraphs and deep learning towards a complete set of genes as the nodes. To connect the biology and deep learning communities, we bring together different sources of gene and function information into one data package. We provide details over what we call the Arabidopsis dataset as well as go over transcriptomic data from Arabidopsis plants we have assembled. We then provide baseline experimental results to showcase how hypergraph models learn correlations amongst gene features to predict the up- or down-regulation with regards to gene expression. We further provide justification for using hypergraphs over graphs for this dataset. Finally, to address the challenges of our dataset, we discuss the experimental results and offer advice for future directions.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="./projects/arabidopsis-thaliana"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1">
                                    <img src="https://cdnsecakmi.kaltura.com/p/1315742/thumbnail/entry_id/1_wsnj0g71/width/720" alt="">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Understanding Nutrition Through Machine Vision</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">Healthy food choices are central to nutritional security, but these choices require individuals to find a complex balance between interests in the present and future. Therefore, policies to encourage healthier eating must consider food characteristics to which people give greater attention when making these intertemporal decisions and the psychological mechanisms through which those characteristics influence the decisions. In this project, we propose to study emotion as one such psychological mechanism.
                                            We propose to utilize cutting-edge machine vision and artificial intelligence technologies to evaluate the role of attention and emotion in food choice. These technologies use cameras to track subjects through a physical space, measure attention through eye-tracking, and measure emotion through facial micro-expressions. We propose three key research questions: 1) How do emotions mediate food choice decisions? 2) How well do emotion and attention predict food choices in a natural environment? And 3) How does emotion interact with context to influence food choice?
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="https://uark-cviu.github.io/projects/nutrition-machine-vision/"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1">
                                    <img src="./projects/video-scene-graph/assets/images/teaser.png" alt="">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Video Scene Graph</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">Video Scene Graph Generation (VidSGG) is a research field focused on converting visual input from video streams into structured knowledge representations.
                                            These scene graphs contain nodes (objects) and edges (relationships) that help machines understand context and interactions within each frame over time.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="https://uark-cviu.github.io/projects/video-scene-graph/"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="projects/quantum-brain/static/images/quantum_mind.jpg" alt="">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Quantum-Brain</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">Human brain, often regarded as one of nature's most intricate computational systems, demonstrates remarkable abilities in information processing, adaptability, and creative problem-solving. Meanwhile, quantum mechanics, with its principles of superposition and entanglement, introduces a groundbreaking perspective on computation and information processing. Quantum-Brain aims investigate whether these two domains intersect in ways that could redefine our understanding of consciousness, decision-making, and even the nature of reality itself. In this project, we focuses on mapping the connections between quantum mechanics and the neural systems of the human brain. Additionally, it leverages quantum theories to develop advanced deep learning models that shed light on brain functionality. By bridging the quantum mechanics with neuroscience, this project represents a step toward decoding the mysteries of the mind and unlocking the next generation of scientific innovation.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="./projects/quantum-brain"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="projects/quantum/img/quantumml/cloud_system.png" alt="">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Quantum Crystals Identification</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">In quantum machine field, detecting two-dimensional (2D) materials in Silicon chips is one of the most critical problems. It is considered as one of bottlenecks in quantum research because of time and labor consumptions spent on finding a potential flake that might be useful. This progress takes hours to finish without any warranty that detected flakes being helpful. In order to speedup, reduce cost and efforts of this progress, we leverage computer vision and AI to build an end-to-end system for automatically identifying potential flakes and exploring their charactersitics (e.g thickness). We provide a flexible and generalized solution for 2D quantum crystals identification running on realtime with high accuracy. The algorithm is able to work with any kind of flakes (e.g hBN, Graphine, etc), hardware and environmental settings. It will help to reduces time and labor consumption in research of quantum technologies.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="./projects/quantum/1_quantumml.html"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="projects/optimization/assets/images/quantum_optimization.png" alt="">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Quantum Optimization and Quantum Machine Learning</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">We have focused on the classic problem of the Capacitated Vehicle Routing Problem (CVRP) because of its real-world industry applications. Heuristics are often employed to solve this problem because it is difficult. In addition, meta-heuristic algorithms have proven to be capable of finding reasonable solutions to optimization problems like the CVRP. Recent research has shown that quantum-only and hybrid quantum/classical approaches to solving the CVRP are possible. Where quantum approaches are usually limited to minimal optimization problems, hybrid approaches have been able to solve more significant problems. Still, the hybrid approaches often need help finding solutions as good as their classical counterparts.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="./projects/optimization/quantum_optimization.html"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="https://campusdata.uark.edu/resources/images/articles/2024-05-17_05-27-21-PM_70485.jpg" alt="">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Thanh-Dat Truong Selected for CVPR 2024 Doctoral Consortium</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">Thanh-Dat Truong, a Ph.D. candidate in the Department of Electrical Engineering and Computer Science, will participate in the Computer Vision and Pattern Recognition Conference Doctoral Consortium. The conference takes place on June 17 in Seattle, Washington, and is one of the top publishing AI conferences.                                            .
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="https://news.uark.edu/articles/70485/thanh-dat-truong-selected-for-cvpr-2024-doctoral-consortium"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/HIG.jpg" alt="Type-to-Track: Retrieve Any Object
                                        via Prompt-based Tracking">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">HIG: Hierarchical Interlacement Graph Approach
                                        to Scene Graph Generation in Video Understanding</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">Visual interactivity understanding within visual scenes presents a significant challenge in computer vision. Existing methods focus on complex interactivities while leveraging a simple relationship model. These methods, however, struggle with a diversity of appearance, situation, position, interaction, and relation in videos. This limitation hinders the ability to fully comprehend the interplay within the complex visual dynamics of subjects. In this paper, we delve into interactivities understanding within visual content by deriving scene graph representations from dense interactivities among humans and objects. To achieve this goal, we first present a new dataset containing Appearance-Situation-Position-Interaction-Relation predicates, named ASPIRe, offering an extensive collection of videos marked by a wide range of interactivities. Then, we propose a new approach named Hierarchical Interlacement Graph (HIG), which leverages a unified layer and graph within a hierarchical structure to provide deep insights into scene changes across five distinct tasks. Our approach demonstrates superior performance to other methods through extensive experiments conducted in various scenarios.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="https://uark-cviu.github.io/ASPIRe/"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="./projects/insect-foundation/imgs/insect_attention.gif" alt="Insect Foundation Model">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Insect Foundation Model</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">Insect-related disasters are one of the most important factors affecting crop yield due to the fast reproduction and widely distributed, large variety of insects. In the agricultural revolution, detecting and recognizing insects plays an important role in the ability for crops to grow healthily and produce a high-quality yield. To achieve this, insect recognition helps to differentiate between bugs that must be targeted for pest control and bugs that are essential for protecting farms. Machine learning, especially deep learning requires a large volume of data to achieve high performance. Therefore, we introduce a novel "Insect Foundation" dataset, a game-changing resource poised to revolutionize insect-related foundation model training. This rich and expansive dataset consists of 900,000 images with dense labels of taxonomy hierarchy from the high level of taxonomy (e.g., Class, Order) to the low level of taxonomy (e.g., Genus, Species). Covering a vast spectrum of insect species, our dataset offers a panoramic view of entomology, enabling foundation models to comprehend visual and semantic information about insects like never before. Our proposed dataset carries immense value, fostering breakthroughs across precision agriculture and entomology research. Insect Foundation Dataset promises to empower the next generation of insect-related AI models, bringing them closer to the ultimate goal of precision agriculture.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="./projects/insect-foundation/"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                           
                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="./assets/img/publications/figures/teaser_mot.png" alt="Type-to-Track: Retrieve Any Object
                                        via Prompt-based Tracking">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Type-to-Track: Retrieve Any Object via Prompt-based Tracking</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">One of the recent trends in vision problems is to use natural language captions to describe the objects of interest. This approach can overcome some limitations of traditional methods that rely on bounding boxes or category annotations. This paper introduces a novel paradigm for Multiple Object Tracking called Type-to-Track, which allows users to track objects in videos by typing natural language descriptions. We present a new dataset for that Grounded Multiple Object Tracking task, called GroOT, that contains videos with various types of objects and their corresponding textual captions describing their appearance and action in detail. Additionally, we introduce two new evaluation protocols and formulate evaluation metrics specifically for this task. We develop a new efficient method that models a transformer-based eMbed-ENcoDE-extRact framework (MENDER) using the third-order tensor decomposition. The experiments in five scenarios show that our MENDER approach outperforms another two-stage design in terms of accuracy and efficiency, up to 14.7% accuracy and 4× speed faster.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="https://uark-cviu.github.io/Type-to-Track/"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>

                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="https://uark-cviu.github.io/projects/quantum/img/quantumml/qnn_2.png" alt="Quantum Machine Learning and Autonomous 2D Crystals Identification">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Quantum Machine Learning and Autonomous 2D Crystals Identification</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">Classical neural network algorithms are computationally expensive. For example, in image classification, representing an image pixel by pixel using classical information requires an enormous amount of computational memory resources. Hence, exploring methods to represent images in a different paradigm of information is important. We proposed a parameter encoding scheme for defining and training neural networks in quantum information based on time evolution of quantum spaces.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="https://uark-cviu.github.io/projects/quantum/1_quantumml.html"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="https://campusdata.uark.edu/resources/images/articles/2023-10-30_09-36-47-PM_66758.jpg" alt="Researchers Receive NSF Funding to Continue Building a Smarter Insect Trap">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Researchers Receive NSF Funding to Continue Building a Smarter Insect Trap</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">Ashley Dowling, a researcher in the U of A System Division of Agriculture and professor of entomology and plant pathology, and Khoa Luu, an assistant professor of computer science and computer engineering, are leading development of the smarter insect trap.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="https://news.uark.edu/articles/66758/researchers-receive-nsf-funding-to-continue-building-a-smarter-insect-trap"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="https://campusdata.uark.edu/resources/images/articles/2023-08-25_04-31-20-PM_64968.jpg" alt="Graduate Students Take Third Place in MIT's Algonauts Project 2023 Challenge">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Graduate Students Take Third Place in MIT's Algonauts Project 2023 Challenge</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">Xuan Bac Nguyen, a Ph.D. candidate in the Department of Electrical Engineering and Computer Science, and his team placed third in the MIT Vision Brain Challenge, Algonauts Project 2023. The competition featuring more than 100 research teams around the world judges how successfully computational AI models predict brain responses to visual stimuli of natural scenes.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="https://news.uark.edu/articles/64968/graduate-students-take-third-place-in-mit-s-algonauts-project-2023-challenge"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="https://campusdata.uark.edu/resources/images/articles/2023-05-17_05-56-06-PM_64422.jpg" alt="Advanced Computer Vision">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Technology Ventures Inventor Spotlight: Khoa Luu</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">Recently Dr. Luu has been working on the problem that manually managing insects seems impossible as the size of a farm gets larger. Therefore, the ability to automatically detect and identify insects is a primary demand in the area of crop management. Despite the many advances in precision agriculture, this is one area still largely reliant on manual labor. A primary objective in this area is the ability to identify and count insect species in real-time.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="https://news.uark.edu/articles/64422/technology-ventures-inventor-spotlight-khoa-luu"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="./assets/img/publications/grid/houston.png" alt="Advanced Computer Vision">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Advanced Computer Vision and Deep
                                        Learning with Limited Data Approaches to Human
                                        Behavior Analysis and Scene Understanding in
                                        Open World</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="docs/houston.pdf"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="./assets/img/publications/grid/public-health-and-tech-speaker.png" alt="public-health-and-tech-speaker">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Advanced Artificial Intelligence and Deep Learning with Limited Data Approaches to Human Behavior Analysis and Health-Care Applications</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="docs/CPHT_KLuu.pdf"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="./assets/img/publications/grid/15290993697098387075.jpeg" alt="FutureFarming">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Short Takes: Caught on Camera: Insects Edition</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">University of Arkansas researchers have developed a prototype of an insect trap that can help farmers monitor and identify potential pests more efficiently in order to protect valuable crops. The trap, developed by researchers Ashley Dowling and Khoa Luu, captures footage of insects, uses artificial intelligence to identify them and sends real-time data back to the farmers. It also eliminates the need for manual monitoring, allowing farmers to make decisions on the fly and take the appropriate measures to counteract potential damage.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="https://www.youtube.com/watch?v=fwIAGTgrixs"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="https://honorsblog.uark.edu/files/2022/10/GrantPhoto-300x170-1.jpg" alt="FutureFarming">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Training AI to Detect Crop Pests</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">Pierce Helton is an Honors student studying Computer Science. This summer, he worked as a research assistant in the department’s Computer Vision and Image Understanding Lab. Here, he had an opportunity to broaden his understanding of machine learning and contribute to department research. Pierce plans to work in the industry and potentially pursue a M.S. in Computer Science after earning his B.S.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="https://honorsblog.uark.edu/training-ai-to-detect-crop-pests/"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="./assets/img/publications/grid/autism.png" alt="FutureFarming">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Researchers using artificial intelligence to assist with early detection of autism spectrum disorder</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">Faculty in food science and computer science/computer engineering are collaborating to develop machine learning that can assist in the detection of autism spectrum disorder.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="https://www.eurekalert.org/news-releases/962494"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="https://campusdata.uark.edu/resources/images/articles/PMUA_Logo-web.jpg" alt="FutureFarming">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">MonArk NSF Quantum Foundry Established With $20 Million Grant</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">With a $20 million grant from the National Science Foundation, the U of A and Montana State University will establish the MonArk NSF Quantum Foundry to accelerate the development of quantum materials and devices.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="https://news.uark.edu/articles/57527/monark-nsf-quantum-foundry-established-with-20-million-grant"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="https://campusdata.uark.edu/resources/images/articles/2020-06-23_02-35-32-PMCIF_1.jpg" alt="FutureFarming">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">Chancellor's Innovation and Collaboration Fund Awards 10 Faculty Research Projects</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">In a continuous effort to promote research activity, ten faculty research projects have been awarded grants from the Chancellor's Innovation and Collaboration Fund.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="https://news.uark.edu/articles/54173/chancellor-s-innovation-and-collaboration-fund-awards-10-faculty-research-projects"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1">
                                    <video controls width="100%"><source src="https://images.fastcompany.net/image/upload/w_1250,ar_16:9,c_fill,g_auto,f_webm,q_70/wp-cms/uploads/2019/03/p-2-Zuckerberg.gif" type="video/webm"></video>
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">This new AI tool ages faces in videos with creepy accuracy</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">A new machine learning paper shows how AI can take footage of someone and duplicate the video with the subject looking an age the researchers specify. The team behind the paper, from the University of Arkansas, Clemson University, Carnegie Mellon University, and Concordia University in Canada, claim that this is one of the first methods to use AI to tackle aging in videos.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="https://www.fastcompany.com/90314606/this-new-ai-tool-makes-creepily-realistic-videos-of-faces-in-the-future"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="https://wordpressua.uark.edu/vision/files/2018/09/Screen-Shot-2018-09-22-at-3.33.39-AM-1n8box2-768x300.png" alt="FutureFarming">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">ECCV 2018 - 2nd Unconstrained Face Detection and Open Set Recognition Challenge</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">We are the Winner #1 in both two tracks: Face Detection and Face Recognition.
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="https://vast.uccs.edu/Opensetface/"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="sp-line-80"></div>
                            <div class="row">
                                <div class="col-md-4 ol-hover hover-1"><img
                                        src="https://wordpressua.uark.edu/vision/files/2018/09/au-1p2wryo-768x366.jpg" alt="FutureFarming">
                                </div>
                                <div class="col-md-8">
                                    <h4 class="m-bottom-30">EmotioNet Challenge 2018</h4>
                                    <div class="dl-horizontal text-justify tight m-bottom-30">
                                        <div class="details">We are the 3rd in Track #2!
                                            <br><br><span style="font-family:Source Code Pro"><a class="link-with-icon"
                                                    href="http://cbcsl.ece.ohio-state.edu/EmotionNetChallenge/index.html#enc-2018"><i
                                                        class="fa fa-link"></i>Link</a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </section>
                </section>
                <div class="clearfix"></div>
            </section>
        </section>
        <footer id="footer" class="dark-wrapper">
            <div id="footer-bar">
                <div class="container">
                    <div class="row tb-vcenter-wrapper bottom-bar">
                        <div class="col-sm-8 vcenter">
                            <ul class="footer-menu">
                                <li><a href="./index.html">Home</a></li>
                                <li><a href="./publication.html">Publication</a></li>
                                <li><a href="https://github.com/uark-cviu">Data & Code</a></li>
                                <li><a href="./news.html"><b>News</b></a></li>
                                <li><a href="./people.html">People</a></li>
                                <li><a href="./contact.html">Contact</a></li>
                            </ul>
                            <div class="sp-blank-10"></div>
                            <div class="inline-wrapper">
                                <div class="copyright"><i class="fa fa-copyright"></i> CVIU Lab
                                    <script>document.write(new Date().getFullYear())</script>.
                                </div>
                            </div>
                        </div>
                        <div class="col-sm-4 vcenter footer-socials">
                            <ul class="social-icons border-circle hover-tb-theme text-right-sm">
                                <li><a href="mailto:khoaluu@uark.edu"><i class="fa fa-envelope"></i></a></li>
                                <li><a href="tel:+14126085708"><i class="fa fa-phone"></i></a></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </footer>
    </div>
    <script src="./assets/js/vendors/vendors.js"></script>
    <script src="./assets/js/custom.js"></script>
</body>

</html>