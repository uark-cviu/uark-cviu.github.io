<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
  <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <link rel="stylesheet" href="jemdoc.css" type="text/css" />
  <title></title>
</head>
<style>
  img {
    display: block;
    margin-left: auto;
    margin-right: auto;
  }
</style>

<body>
  <h1 style="color:steelblue;" align="center">Autonomous Driving and Perception</h1>

  <div align="center">
    <div style="font-size:20px;">
      CVIU Lab, Department of Computer Science and Computer Engineering
      <br>
      University of Arkansas
    </div>
    <!-- Contacts: 
    <br> -->
    <a href="https://uark-cviu.github.io/">https://uark-cviu.github.io/</a>
    <br>
    Email: khoaluu@uark.edu
  </div>
  <br>
  <img src="imgs/av_drone_segmentation.jpg" alt="Autonomous Driving and Perception" width="500px">

  <h2 style="color:steelblue;">Introduction</h2>

  <div align="justify">
    Autonomous driving has emerged as a transformative technology in the automotive industry, promising to revolutionize the
    way we move around in our daily lives. At its core, autonomous driving relies on a sophisticated system of sensors and
    algorithms to perceive the environment and make intelligent decisions based on that information.
    
    Perception, in particular, is a critical aspect of autonomous driving, as it enables the vehicle to sense and interpret
    its surroundings in real-time. This involves processing data from a range of sensors, including cameras, lidar, radar,
    and GPS, and using machine learning algorithms to identify and track objects such as other vehicles, pedestrians, and
    road signs.
    
    In this project, we aim to explore the current state-of-the-art in autonomous driving and perception, including the key
    technologies, challenges, and opportunities for future innovation. We will delve into the technical details of how these
    systems work and examine real-world applications of autonomous driving, such as self-driving cars, trucks, and drones.
    
    By gaining a deeper understanding of the principles behind autonomous driving and perception, we hope to contribute to the ongoing development of this exciting field and pave the way for a safer, more efficient, and more connected future on the roads.
    <div>
      <br>
      <br>
      <div align="center">

        <iframe src="https://drive.google.com/file/d/1PKndwezRCdtC23vo9sfVzZUCzafQI30Z/preview" width="920" height="540" 
          allow="autoplay"></iframe>

      </div>
      <br>
      <br>

      <h2 style="color:steelblue;">Our Solution</h2>
      Our CVIU Lab focuses on four major technologies of autonomous driving:
      <p><em>1. Unsupervised and Self-supervised Approach to Semantic Segmentation.</em> We aim to build the perception
        model
        without using real-world labeled data.</p>
      <p><em>2. Continuous Learning Approach to Semantic Segmentation.</em> We aim to build a perception model that is
        able to
        continuously update the knowledge with respect to the deployment scenarios.</p>
      <p><em>3. Robust Detection and Tracking.</em> We aim to build a robust detection and tracking module that allows
        the vehicles can
        monitor their surrounding objects.</p>
      <p><em>4. Driving Monitoring System.</em> We aim to build a robust monitoring system to guarantee the safety of
        drivers
        by monitoring the drivers' behavior.</p>

      <!-- <img center="align" src="imgs/av_segmentation.png" width=1000px"> -->

      <table align="center">
        <tr align="center">
          <td align="center">
            <img src="./imgs/conda.png" height="300px">
          </td>
          <td align="center">
            <img src="./imgs/bimal.png" height="300px">
          </td>
          <td align="center">
            <img src="./imgs/graph_mcmot.png" height="300px">
          </td>
        </tr>
      </table>


      <h2 style="color:steelblue;">Our Team</h2>
      <table align="center">
        <tr align="center">
          <td align="center">
            <img src="./imgs/khoaluu.jpg" width=224px height="350px">
          </td>
          <td align="center">
            <img src="./imgs/DatTruong.png" width=224px height="350px">
          </td>
          <td align="center">
            <img src="./imgs/PhaNguyen.jpg" width=224px height="350px">
          </td>
          <td align="center">
            <img src="./imgs/bacnguyen.png" width=224px height="350px">
          </td>
        </tr>
        <tr align="center">
          <td align="center"><a href="http://csce.uark.edu/~khoaluu/">Dr. Khoa Luu</a></td>
          <td align="center"><a href="https://truongthanhdat.github.io/">Thanh Dat Truong</a></td>
          <td align="center"><a href="https://pha-nguyen.github.io/">Pha Nguyen</a></td>
          <td align="center"><a href="https://ngxbac.github.io/">Xuan Bac Nguyen</a></td>
        </tr>
        <tr align="center">
          <td align="center">Assistant Professor</td>
          <td align="center">Ph.D. Candidate</td>
          <td align="center">Ph.D. Student</td>
          <td align="center">Ph.D. Student</td>
        </tr>
        <tr align="center">
          <td align="center">khoaluu@uark.edu</td>
          <td align="center">tt032@uark.edu</td>
          <td align="center">panguyen@uark.edu</td>
          <td align="center">xnguyen@uark.edu</td>
        </tr>
      </table>


      <h2 style="color:steelblue;">Publication</h2>
      <p>[1] Thanh-Dat Truong, Ngan Le, Bhiksha Raj, Jackson Cothren, and Khoa Luu (2023). FREDOM: Fairness Domain Adaptation
        Approach to Semantic Scene Understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
        Recognition (CVPR), 2023. </p>

      <p>[2] Thanh-Dat Truong, Quoc-Huy Bui, Chi Nhan Duong, Han-Seok Seo, Son Lam Phung, Xin Li, and Khoa Luu (2022). DirecFormer: A
        Directed Attention in Transformer Approach to Robust Action Recognition. In Proceedings of the IEEE/CVF Conference on
        Computer Vision and Pattern Recognition (CVPR), 2022.</p>

      <p>[3] Pha Nguyen Kha Gia Quach, Chi Nhan Duong, Ngan Le, Xuan-Bac Nguyen, Khoa Luu (2022). Multi-Camera Multiple 3D Object
          Tracking on the Move for Autonomous Vehicles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
          Recognition Workshop (CVPRW), 2022.</p>

      <p>[4] Pha Nguyen, Kha Gia Quach, Chi Nhan Duong, Son Lam Phung, Ngan Le, and Khoa Luu (2022). Multi-Camera Multi-Object Tracking on
        the Move via Single-Stage Global Association Approach. arXiv, 2022. </p>

      <p>[5] Thanh-Dat Truong, Pierce Helton, Ahmed Moustafa, Jackson David Cothren, and Khoa Luu (2022). CONDA: Continual
        Unsupervised Domain Adaptation Learning in Visual Perception for Self-Driving Cars. arXiv, 2022. </p>

      <p>[6] Thanh-Dat Truong, Chi Nhanh Duong, Pierce Helton, Ashley Dowling, Xin Li, and Khoa Luu (2022). CoMaL: Conditional Maximum
          Likelihood Approach to Self-supervised Domain Adaptation in Long-tail Semantic Segmentation. IEEE Transactions on
          Pattern Analysis and Machine Intelligence (Under Review).</p>

      <p>[7] Kha Gia Quach, Huu Le, Pha Nguyen, Chi Nhan Duong, Tien Dai Bui, and Khoa Luu (2022). Depth Perspective-aware
        Multiple Object Tracking. arXiv, 2022.</p>

      <p>[8] Thanh-Dat Truong, Chi Nhan Duong, Ngan Le, Son Lam Phung, Chase Rainwater, Khoa Luu (2021). BiMal: Bijective Maximum
        Likelihood Approach to Domain Adaptation in Semantic Scene Segmentation. Proceedings of the IEEE/CVF International
        Conference on Computer Vision (ICCV), 2021.</p>
      
      <p>[9] Kha Gia Quach, Pha Nguyen, Huu Le, Thanh-Dat Truong, Chi Nhan Duong, Minh-Triet Tran, and Khoa Luu (2021). DyGLIP: A Dynamic
        Graph Model with Link Prediction for Accurate Multi-Camera Multiple Object Tracking. In Proceedings of the IEEE/CVF
        Conference on Computer Vision and Pattern Recognition (CVPR), 2021. </p>

      <h2 style="color:steelblue;">Sponsors</h2>


      <table align="center">
        <tr align="center">
          <td align="center">
            <img src="./imgs/nsf.png" height="350px">
          </td>
          <td align="center">
            <img src="./imgs/dart.png" height="350px">
          </td>
        </tr>
      </table>




</body>

</html>