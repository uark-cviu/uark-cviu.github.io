
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CYCLO</title>
  
  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="static/images/UA_Logo.png">
  
  <!-- Fonts and Icons -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  
  <!-- CSS Libraries -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
  
  <!-- Google Analytics -->
  <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'G-825ET94Y3E');
  </script>
  
  <!-- JavaScript Libraries -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
  
<body>
  <section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">CYCLO <img src="static/images/cyclo.png" alt="Icon" style="width: 64px; height: 52px;">: Cyclic Graph Transformer Approach to Multi-Object Relationship Modeling in Aerial Videos</h1>
                    <div class="is-size-5 publication-authors">
                      <!-- Paper authors -->
                      <span class="author-block">
                      <a href="https://scholar.google.com/citations?user=ty0Njf0AAAAJ&hl=vi&authuser=1/" target="_blank"> Trong-Thuan Nguyen<sup>1</sup></a>,</span>
                      <a href="https://pha-nguyen.github.io/" target="_blank"> Pha Nguyen<sup>1</sup></a>,</span>
                      <a href="https://scholar.google.com/citations?user=gMBvzGoAAAAJ&hl=vi" target="_blank"> Xin Li<sup>2</sup></a>,</span><br>
                      <a href="https://scholar.google.com/citations?user=_WB9fo4AAAAJ&hl=vi&oi=ao" target="_blank"> Jackson Cothren<sup>1</sup></a>,</span>
                      <a href="https://scholar.google.com/citations?user=MeQC1XYAAAAJ&hl=vi&oi=ao" target="_blank"> Alper Yilmaz<sup>3</sup></a>,</span>
                      <a href="https://scholar.google.com/citations?user=JPAl8-gAAAAJ" target="_blank"> Khoa Luu<sup>1</sup></a></span>
                      </span>
                  </div>
                  <div class="is-size-5 publication-authors">
                      <span class="author-block">
                        <sup>1 </sup>University of Arkansas&nbsp;&nbsp;&nbsp;&nbsp;
                        <sup>2 </sup>State University of New York at Albany&nbsp;&nbsp;&nbsp;&nbsp;
                        <sup>3 </sup>Ohio State University&nbsp;&nbsp;&nbsp;&nbsp;
                      </span>
                      <br>
                  </div>
                  <p style="color:#000; font-weight:bold; font-size:1.5em;" class="text-center">ðŸŽ‰ <strong>Accepted to NeurIPS 2024</strong> ðŸŽ‰</p>
                    <!-- Publication Links -->
                    <div class="publication-links">
                        <!-- ArXiv abstract Link -->
                        <span class="link-block">
                            <a href="https://arxiv.org/abs/2406.01029" target="_blank" class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                    <i class="ai ai-arxiv"></i>
                                </span>
                                <span>arXiv</span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a href="#annotations" target="_self" class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                    <i class="fas fa-download"></i>
                                </span>
                                <span>Dataset</span>
                            </a>
                        </span>
                    </div>
                    <!-- Video Container -->
                    <div class="video-container">
                      <video autoplay muted loop playsinline>
                          <source src="static/videos/teaser.mp4" type="video/mp4">
                      </video>
                      <ul><i>The videos on AeroEye are collected from <a href="https://mavrec.github.io/">MAVREC</a> and <a href="https://lcmou.github.io/ERA_Dataset/">ERA</a>, which are protected under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> license.</i></ul>
                  </div>
                </div>
            </div>
        </div>
    </div>
</section>


<!-- Paper motivation -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Highlights</h2>
        <div class="content has-text-justified">
          <p>
            <ul>
              <li>We introduce a new <b>AeroEye</b> dataset for VidSGG in drone videos, augmented with numerous predicates and diverse scenes to capture the complex relationships in aerial videos.</li>
              <li>We propose the <b>CYCLO</b> approach, utilizing circular connectivity among frames to enable periodic and overlapping relationships. It allows the model to capture long-range dependencies and process object interactions in the appropriate temporal arrangement.</li>
              <li>The proposed <b>CYCLO</b> approach outperforms prior methods on two large-scale in-the-wild VidSGG datasets, including OpenPVSG and ASPIRe.</li>
            </ul>
          </p>
        </div>
      </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
            <p> Video scene graph generation (VidSGG) has emerged as a transformative approach to capturing and interpreting the intricate relationships among objects and their temporal dynamics in video sequences. 
                In this paper, we introduce the new AeroEye dataset that focuses on multi-object relationship modeling in aerial videos. Our AeroEye dataset features a rich variety of drone scenes and includes a comprehensive collection of predicates that capture the intricate relationships and spatial arrangements among objects. 
                To this end, we propose the novel Cyclic Graph Transformer (CYCLO) approach that allows the model to capture both direct and long-range temporal dependencies by continuously updating the history of interactions in a circular manner. 
                The proposed approach also allows one to handle sequences with inherent cyclical patterns and process object relationships in the correct sequential order. 
                Therefore, it can effectively capture periodic and overlapping relationships while minimizing information loss. 
                The extensive experiments on the AeroEye dataset demonstrate the effectiveness of the proposed CYCLO model, demonstrating its potential to perform scene understanding on drone videos. 
                Finally, the CYCLO method consistently achieves State-of-the-Art (SOTA) results on two in-the-wild benchmarks, i.e., OpenPVSG and ASPIRe.
            </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Results</h2>
        <div class="columns is-centered has-text-centered">
          <div class="content has-text-justified">
            <div class="video-container">
            </div>
            <div class="videos">
              <video id="video1" src="static/videos/PoliceChase_002.mp4" type="video/mp4" autoplay muted loop playsinline></video>
              <video id="video2" src="static/videos/Cycling_005.mp4" type="video/mp4" autoplay muted loop playsinline></video>
              <video id="video3" src="static/videos/Constructing_007.mp4" type="video/mp4" autoplay muted loop playsinline></video>
              <video id="video4" src="static/videos/Soccer_054.mp4" type="video/mp4" autoplay muted loop playsinline></video>
              <video id="video5" src="static/videos/Constructing_058.mp4" type="video/mp4" autoplay muted loop playsinline></video>
            </div>
            <p class="caption"><strong>Scene graphs generated by the CYCLO model on the AeroEye dataset.</strong></p>
            <div class="video-navigation">
              <button id="prevBtn"><i class="fas fa-arrow-circle-left"></i> Previous</button>
              <button id="nextBtn">Next <i class="fas fa-arrow-circle-right"></i></button>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section", id="annotations">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">The AeroEye Dataset</h2>
        <div class="content has-text-justified">
          AeroEye includes videos from the <a href="https://mavrec.github.io/">MAVREC</a> and <a href="https://lcmou.github.io/ERA_Dataset/">ERA</a> 
          along with our own object <span style="background-color: #c4d544"><b>detection</b></span>, 
          <span style="background-color: #c0bffe"><b>tracking</b></span>, and 
          <span style="background-color: #f5b480"><b>relationship</b></span> instance annotations. You can download the videos here:
          <ul>
            <li><a href="https://mavrec.github.io/" class="mavrec-link">MAVREC</a></li>
            <li><a href="https://lcmou.github.io/ERA_Dataset/" class="era-link">ERA</a></li>
          </ul>
          <p>
            <strong>AeroEye v1.0</strong>:
          </p>
          <ul>
            <li>
              The <a href="https://drive.google.com/file/d/1933ZGScwyZbNaSb0yusX9klhmXHDk24p/view?usp=sharing" target="_blank">annotations</a> are available for download.
            </li>
            <li>
              The <a href="https://drive.google.com/drive/folders/19cfOOiqd1zX1uDUym215k317_k4eWlxx?usp=sharing" target="_blank">10-fold cross-validation</a> splits are also provided.
            </li>
          </ul>          
      </div>
        <div class="columns">
            <table class="table">
                <thead>
                    <tr>
                        <th><span class="number">2.3K</span><span class="text"> videos</span></th>
                        <th><span class="number">261.5K</span><span class="text"> frames</span></th>
                        <th><span class="number">2.2M</span><span class="text"> bounding boxes</span></th>
                        <th><span class="number">43M</span><span class="text"> relationship instances</span></th>
                    </tr>
                </thead>
            </table>
        </div>   
        <figure style="display: inline-block; text-align: center; width: 55%; margin-right: 5%;">
            <img class="img-zoom" src="static/images/data_stats.png" style="width: 100%;" />
            <figcaption>Statistics for each scene on AeroEye dataset.</figcaption>
          </figure>
          <figure style="display: inline-block; text-align: center; width: 35%;">
            <img class="img-zoom" src="static/images/word_cloud.png" style="width: 100%;" />
            <figcaption>Relationship word cloud on AeroEye dataset</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
      <div class="columns is-centered">
          <div class="column">
              <h2 class="title is-3 has-text-centered">Comparison</h2> <img src="static/images/compare.png" style="display: block; margin-left: auto; margin-right: auto" />
          </div>
      </div>
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>

        <div class="content has-text-left is-size-5">
        <strong>Results on AeroEye Dataset</strong>
        </div>
        <figure>
            <figcaption class="content has-text-left" style="word-break:normal">Our CYCLO apporach is compared on three task Predicate Classification (<i><b>PredCls</b></i>), Scene Graph Classification (<i><b>SGCls</b></i>) and Scene Graph Detection (<i><b>SGDet</b></i>).
            <figcaption class="content has-text-left" style="word-break:normal"><li>Comparison (mean &plusmn; std) against baseline methods at Recall (R). The best results are in <b>bold</b>.</li>
            <img src="static/images/recall_aeroeye.png" alt="fail" width="100%">
        </figure>
        <figure>
            <figcaption class="content has-text-left" style="word-break:normal"><li>Comparison (mean &plusmn; std) against baseline methods at mean Recall (mR). The best results are in <b>bold</b>.</li>
            <img src="static/images/mrecall_aeroeye.png" alt="fail" width="100%">
          </figure>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Results on PVSG Dataset</strong>
        </div>
        <figure>
            <figcaption class="content has-text-left" style="word-break:normal"><li>Comparative performance (%) of our model and previous methods on the PVSG dataset, evaluated by Recall (R) and mean Recall (mR). The best results are in <b>bold</b>.</li>
            <img src="static/images/pvsg.png" alt="fail" width="100%">
        </figure>
        <br>
        <div class="content has-text-left is-size-5">
            <strong>Results on PVSG Dataset</strong>
          </div>
          <figure>
            <figcaption class="content has-text-left" style="word-break:normal"><li>Comparative performance (%) of our model and previous methods on the ASPIRe dataset, evaluated by Recall (R) and mean Recall (mR). The best results are in <b>bold</b>.</li>
            <img src="static/images/aspire.png" alt="fail" width="100%">

          </figure>
      </div>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://githubcom/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>
  <!-- Initialize Slick Carousel -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
        let currentVideoIndex = 0;
        const videos = document.querySelectorAll('.videos video');
        const totalVideos = videos.length;

        document.getElementById('prevBtn').addEventListener('click', function() {
            changeVideo(-1);
        });

        document.getElementById('nextBtn').addEventListener('click', function() {
            changeVideo(1);
        });

        document.querySelectorAll('.img-zoom').forEach(img => {
            img.addEventListener('click', function() {
                this.classList.toggle('active');
            });
        });

        function changeVideo(direction) {
            videos[currentVideoIndex].style.display = 'none';
            currentVideoIndex = (currentVideoIndex + direction + totalVideos) % totalVideos;
            videos[currentVideoIndex].style.display = 'block';
        }

        function openModal(img) {
            var modal = document.getElementById("myModal");
            var modalImg = document.getElementById("img01");
            modal.style.display = "block";
            modalImg.src = img.src;
        }

        var span = document.getElementsByClassName("close")[0];
        if (span) {
            span.onclick = function() { 
                var modal = document.getElementById("myModal");
                modal.style.display = "none";
            }
        }
    });
  </script>
</body>
</html>
