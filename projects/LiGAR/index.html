<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="LiGAR leverages LiDAR data as a structural backbone to guide the processing of visual and textual information.">
  <meta property="og:title" content="LiGAR"/>
  <meta property="og:description" content="LiGAR leverages LiDAR data as a structural backbone to guide the processing of visual and textual information."/>
  <meta property="og:url" content="https://uark-cviu.github.io/projects/LiGAR/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="LiGAR">
  <meta name="twitter:description" content="https://uark-cviu.github.io/projects/LiGAR/">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>LiGAR</title>
  <link rel="icon" type="image/x-icon" href="static/images/lidar.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LiGAR: LiDAR-Guided Hierarchical Transformer for Multi-Modal Group Activity Recognition</h1>
                  <!-- Paper authors -->
                  
                <div class="is-size-5 publication-authors">
                  <span class="author-block"> <a href="https://nchappa.github.io" target="_blank">Naga VS Raviteja Chappa</a> and </span>
                    <span class="author-block"> <a href="https://scholar.google.com/citations?user=JPAl8-gAAAAJ" target="_blank">Khoa Luu</a></span>
                </div>
                <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Arkansas &nbsp; </span>
                </div>
                <div class="is-size-5 publication-authors">
                  <span class="author-block"> <a href="https://uark-cviu.github.io/" target="_blank">https://uark-cviu.github.io/</a></span>
              </div>
                <div style="font-size: 24px; font-weight: normal; color: black; padding: 20px; border-radius: 5px;">
                    <strong>WACV 2025</strong> &nbsp; 
                </div>
                
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2410.21108" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->
                  <!-- Link Block for Stats Section -->
<!-- <span class="link-block"> 
  <a href="#stats" target="_self" class="external-link button is-normal is-rounded"> 
    <span class="icon"> 
      <i class="fas fa-chart-bar"></i> 
    </span> 
    <span>Statistics</span> 
  </a> 
</span> -->


                  <!-- <span class="link-block"> <a href="#annotations" target="_self" class="external-link button is-normal is-rounded"> <span class="icon"> <i class="fas fa-download"></i> </span> <span>Dataset</span> </a> </span> -->
                
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/uark-cviu/LiGAR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Video carousel -->
 <!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
      <h2 class="title is-3">Dataset Samples with Annotations </h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" controls muted height="60%">
             Your video file here -->
             <!-- <source src="static/videos/visualized_video_6828728630361.mov"
            type="video/mp4">
          </video>
          <div class="is-size-5">Video Source: <a href="https://www.tiktok.com/@chlo_9871/video/6828728630361869574" target="_blank">@chlo_9871</a></div>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="60%">
             Your video file here -->
             <!-- <source src="static/videos/visualized_video_6811718707497241861.mov"
            type="video/mp4">
          </video>
          <div class="is-size-5">Video Source: <a href="https://www.tiktok.com/@x_ironkarma_x/video/6811718707497241861" target="_blank">@x_ironkarma_x</a></div>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="60%">\ -->
            <!-- Your video file here -->
             <!-- <source src="static/videos/visualized_video_7193173231409450266.mov"
            type="video/mp4">
          </video>
          <div class="is-size-5">Video Source: <a href="https://www.tiktok.com/@bangfebri207/video/7193173231409450266" target="_blank">@bangfebri207</a></div>
        </div>
      </div>
    </div>
  </div>
    </div>
  </div> -->
<!-- </section>  -->
<!-- End video carousel -->

<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src=""
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Video coming soon
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Group Activity Recognition (GAR) remains challenging in computer vision due to the complex nature of multi-agent interactions. This paper introduces LiGAR, a LIDAR-Guided Hierarchical Transformer for Multi-Modal Group Activity Recognition. LiGAR leverages LiDAR data as a structural backbone to guide the processing of visual and textual information, enabling robust handling of occlusions and complex spatial arrangements. Our framework incorporates a Multi-Scale LIDAR Transformer, Cross-Modal Guided Attention, and an Adaptive Fusion Module to integrate multi-modal data at different semantic levels effectively. LiGAR's hierarchical architecture captures group activities at various granularities, from individual actions to scene-level dynamics. Extensive experiments on the JRDB-PAR, Volleyball, and NBA datasets demonstrate LiGAR's superior performance, achieving state-of-the-art results with improvements of up to 10.6% in F1-score on JRDB-PAR and 5.9% in Mean Per Class Accuracy on the NBA dataset. Notably, LiGAR maintains high performance even when LiDAR data is unavailable during inference, showcasing its adaptability. Our ablation studies highlight the significant contributions of each component and the effectiveness of our multi-modal, multi-scale approach in advancing the field of group activity recognition.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Highlights -->
<section class="section hero is-light has-background-white">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Highlights</h2>
        <div class="content has-text-justified">
          <ul>
            <li>We propose a <b>novel hierarchical transformer architecture</b> that leverages LiDAR data as a structural backbone to guide the processing of visual and textual information.</li>
        </ul>
        <ul>
          <li> We introduce an <b> Adaptive Fusion Module</b> (AFM) that dynamically integrates LiDAR, visual and textual modalities while modeling temporal dependencies.</li>
        </ul>
        <ul>
          <li> Through extensive experiments on diverse benchmark datasets, including JRDB-PAR, Volleyball, and NBA, we <b> LiGAR's superiority in multi-modal group activity recognition</b>.</li>
        </ul>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End highlights -->


<!-- Motivation -->
<section class="section hero is-small" id="motivation-figure">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-one">
        <h2 class="title is-3 has-text-centered">Motivation</h2>
        <div class="columns is-centered image-container">
          <div class="column is-half">
            <img src="static/images/ligar_motivation.png" alt="motivation-figure" class="small-zoom-img">
            <div class="image-caption">Comparison of LiGAR with conventional GAR methods</div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End of motivation figure -->

<!-- Framework -->
<section class="section hero is-small has-background-white" id="framework-figure">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-one">
        <h2 class="title is-3 has-text-centered">Proposed Framework</h2>
        <div class="columns is-centered image-container">
          <div class="column is-half">
            <img src="static/images/ligar_framework.png" alt="framework-figure" class="small-zoom-img">
            <div class="image-caption">Overview of the LiDAR-Guided Hierarchical Transformer (LiGAR) framework.</div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End of framework figure -->

<!-- Statistics -->
<!-- <section class="section hero is-small" id="stats">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-one">
        <h2 class="title is-3 has-text-centered">Dataset Statistics</h2>
        <p class="content has-text-centered">
          The Public Health Advocacy Dataset (PHAD) includes videos from YouTube and TikTok along with detailed metadata such as user engagement metrics, video descriptions, and search keywords.
        </p>
        <div class="columns is-multiline is-centered stats-container">
          <div class="column is-one-quarter">
            <div class="box stats-box">
              <p class="stats">5.7K</p>
              <p class="stat-label">videos</p>
            </div>
          </div>
          <div class="column is-one-quarter">
            <div class="box stats-box">
              <p class="stats">4.3M</p>
              <p class="stat-label">frames</p>
            </div>
          </div>
          <div class="column is-one-quarter">
            <div class="box stats-box">
              <p class="stats">20M</p>
              <p class="stat-label">engagement metrics</p>
            </div>
          </div>
        </div>
        <div class="columns is-centered image-container">
          <div class="column is-one-third">
            <div class="image-container">
              <img src="static/images/stats/engagement_metrics.png" alt="Description 1" class="zoom-img">
            </div>
            <div class="image-caption">User engagement statistics for each tobacco product on PHAD dataset</div>
          </div>
          <div class="column is-one-third">
            <div class="image-container">
              <img src="static/images/stats/wordcloud.png" alt="Description 2" class="zoom-img">
            </div>
            <div class="image-caption">Search keywords word cloud on PHAD dataset</div>
          </div>
        </div>
        <div class="columns is-centered image-container">
          <div class="column is-full">
            <img src="static/images/stats/comparison.png" alt="Description 3" class="no-zoom-img">
            <div class="image-caption">Additional statistics for PHAD dataset</div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->


<!-- End of Statistics -->


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item"> -->
      <!-- Your image here -->
      <!-- <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->

<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->

<!-- Annotations section -->
<!-- <section class="section hero is-light has-background-white" id="annotations">
  <div class="container is-max-desktop">
      <div class="columns is-centered">
          <div class="column is-one">
              <h2 class="title is-3 has-text-centered">Annotations</h2>
              <div class="column">
                  <h2 class="title is-5"> v1.0: </h2>
                  <div class="content">
                      <a href="annotations/train_annotations.zip" target="_blank">[Train annotations]</a>, 
                      <a href="annotations/val_annotations.zip" target="_blank">[Val annotations]</a>
                  </div>
                  <h2 class="title is-5"> Notes: </h2>
                  <div class="content">
                      <ul>
                          <li>Please follow the data format and the dataset split ratio which are clearly and comprehensively mentioned in the <b>supplementary material</b> to have better understanding of the dataset.</li>
                      </ul>
                  </div>
                  <h2 class="title is-5"> Licensing: </h2>
                  <div class="content"> The annotations of <i>PHAD</i>, are released under a <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" target="_blank">CC BY-NC-SA 3.0</a> license. The original video links provided in the dataset are adhered to the copyrights of <a href="https://www.youtube.com/howyoutubeworks/policies/copyright/" target="_blank">&copy YouTube</a> and <a href="https://www.tiktok.com/legal/page/us/terms-of-service/en" target="_blank">&copy TikTok</a>. </div>
              </div>
          </div>
      </div>
  </div>
</section> -->

<!-- <section class="section hero is-small" id="annotations">
  <div class="container is-max-desktop">
      <div class="columns is-centered">
          <div class="column is-one">
              <h2 class="title is-3 has-text-centered"> Download the PHAD Dataset</h2>
              <div class="column">
                  <h2 class="title is-5"> v1.0: </h2>
                  <div class="content"><a href="annotations/train_annotations.zip" target="_blank">[Train Set]</a>, 
                    <a href="annotations/val_annotations.zip" target="_blank">[Validation Set]</a>
                  </div>
                  <h2 class="title is-5"> Notes: </h2>
                  <div class="content">
                      <ul>
                        <li>Please follow the data format and the dataset split ratio which are clearly and comprehensively mentioned in the <b>supplementary material</b> to have better understanding of the dataset. Also, we included the python scripts to download and organize the dataset in the <b>supplementary material</b>. After the decision of the paper, we will release the github reporsitory with all the corresponding python scripts.</li>
                      </ul>
                  </div>
                  <h2 class="title is-5"> Licensing: </h2>
                  <div class="content"> The <i>PHAD</i> dataset is released under a <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> license. The original video links provided in the dataset are adhered to the copyrights of <a href="https://www.youtube.com/howyoutubeworks/policies/copyright/" target="_blank">&copy YouTube</a> and <a href="https://www.tiktok.com/legal/page/us/terms-of-service/en" target="_blank">&copy TikTok</a>. </div>
              </div>
          </div>
      </div>
  </div>
</section> -->



<!-- Experimental Results -->
<section class="section hero is-small" id="experimental-results">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-one">
        <h2 class="title is-3 has-text-centered">Experimental Results</h2>
        <div class="columns is-centered image-container">
          <div class="column is-half">
            <img src="static/images/results/jrdb.png" alt="Experimental Result 1" class="small-zoom-img">
            <div class="image-caption">Comparison with SOTA on JRDB-PAR dataset.</div>
          </div>
        </div>
        <div class="columns is-centered image-container">
          <div class="column is-full">
            <img src="static/images/results/volleyball_nba.png" alt="Experimental Result 2" class="small-zoom-img">
            <div class="image-caption">Comparison with SOTA on Volleyball and NBA datasets. Here, <strong>WS:</strong> Weakly Supervised.</div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End of experimental results -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item"> 
        <!-- Your image here -->
        <img src="static/images/results/tsne_ligar.drawio.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          <b>t-SNE visualization</b> of video representation on the given datasets learned by <b>different variants of our LiGAR model</b>.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/results/tsne_ligar_supp.drawio.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          <b>t-SNE visualization</b> of video representation on the given datasets learned by <b>different combinations of modalities by our LiGAR model</b>.
        </h2>
      </div>
      <!-- <div class="item">
        Your image here
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      Your image here
      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div> -->
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- References Section -->
<!-- <section class="section hero is-small" id="references">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-one">
        <h2 class="title is-3 has-text-centered">References</h2>
        <div class="content has-text-justified">
          <ol>
            <li data-custom-number="[10]">
              Dhiraj Murthy, Rachel R Ouellette, Tanvi Anand, Srijith Radhakrishnan, Nikhil C Mohan, Juhan Lee, and Grace Kong. 
              "<strong> Computer Vision to Detect E-cigarette Content in TikTok Videos</strong>." 
              Nicotine & Tobacco Research, 26(Supplement_1):S36–S42, 02 2024.
            </li>
            <li data-custom-number="[11]">
              Julia Vassey, Chris J Kennedy, Ho-Chun Herbert Chang, Ashley S Smith, and Jennifer B Unger. 
              "<strong>Scalable Surveillance of E-Cigarette Products on Instagram and TikTok Using Computer Vision</strong>." 
              Nicotine &Tobacco Research, 26(5):552–560, 11 2023.
            </li>
            <li data-custom-number="[27]">
              Sepp Hochreiter and Jürgen Schmidhuber. 
              "<strong>Long short-term memory</strong>." 
              Neural computation, 9(8):1735–1780, 1997.
            </li>
            <li data-custom-number="[28]">
              Joao Carreira and Andrew Zisserman. 
              "<strong>Quo vadis, action recognition? a new model and the kinetics dataset</strong>."
              In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299–6308, 2017.
            </li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- Our Related Works Section -->
<section class="section hero is-small has-background-white" id="references">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-one">
        <h2 class="title is-3 has-text-centered">Prior Works</h2>
        <div class="content has-text-justified">
          <ol>
            <li data-custom-number="[1]">
              Chappa, Naga VS, Pha Nguyen, Alexander H. Nelson, Han-Seok Seo, Xin Li, Page Daniel Dobbs, and Khoa Luu. 
              "<strong>SPARTAN: Self-supervised spatiotemporal transformers approach to group activity recognition</strong>." 
              In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5158-5168. 2023. [<a href="https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Chappa_SPARTAN_Self-Supervised_Spatiotemporal_Transformers_Approach_to_Group_Activity_Recognition_CVPRW_2023_paper.html">Paper</a>]
            </li>
            <li data-custom-number="[2]">
              Chappa, Naga VS, Pha Nguyen, Alexander H. Nelson, Han-Seok Seo, Xin Li, Page Daniel Dobbs, and Khoa Luu. 
              "<strong>SOGAR: Self-supervised spatiotemporal attention-based social group activity recognition</strong>." 
              Under review. [<a href="https://arxiv.org/abs/2305.06310">Paper</a>]
            </li>
            <li data-custom-number="[3]">
              Chappa, Naga VS Raviteja, Pha Nguyen, Page Daniel Dobbs, and Khoa Luu. 
              "<strong>REACT: Recognize Every Action Everywhere All At Once</strong>." 
              Machine Vision and Applications 35, no. 4 (2024): 102. [<a href="https://link.springer.com/article/10.1007/s00138-024-01561-z">Paper</a>]
            </li>
            <li data-custom-number="[4]">
              Chappa, Naga Venkata Sai Raviteja, Pha Nguyen, Thi Hoang Ngan Le, Page Daniel Dobbs, and Khoa Luu. 
              "<strong>HAtt-Flow: Hierarchical Attention-Flow Mechanism for Group-Activity Scene Graph Generation in Videos</strong>." 
              Sensors 24, no. 11 (2024): 3372. [<a href="https://www.mdpi.com/1424-8220/24/11/3372">Paper</a>]
            </li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- End of experimental results -->
<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
